{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vidyacheekuri/concrete_compressive_strength/blob/main/catboost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56ET9OFxQLMc",
        "outputId": "eabb37c4-9c2f-49aa-d497-ededd3409369"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.4 colorlog-6.9.0 optuna-4.5.0\n"
          ]
        }
      ],
      "source": [
        "! pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "g1sFjN4rQLop"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "from sklearn.feature_selection import SelectFromModel, RFE, mutual_info_regression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.linear_model import Ridge\n",
        "import copy\n",
        "from copy import deepcopy\n",
        "import json\n",
        "import optuna\n",
        "import logging\n",
        "import joblib\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.stats import ttest_ind\n",
        "warnings.filterwarnings('ignore')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5fmvql1mbmn",
        "outputId": "393bda03-0b1d-42eb-8375-a8c2b17fa295"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.2\n",
            "\u001b[33mWARNING: Skipping catboost as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ],
      "source": [
        "# First check your numpy version\n",
        "import numpy as np\n",
        "print(np.version.version)\n",
        "\n",
        "# Then reinstall CatBoost\n",
        "!pip uninstall -y catboost\n",
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "B7quAlv1Pb8L"
      },
      "outputs": [],
      "source": [
        "class EnhancedCatBoostPredictor:\n",
        "    \"\"\"Advanced predictor with deeper CatBoost, strength-specific models, and non-linear ensemble.\"\"\"\n",
        "\n",
        "    def __init__(self, random_state=42):\n",
        "        self.random_state = random_state\n",
        "        self.setup_logging()\n",
        "        np.random.seed(random_state)\n",
        "\n",
        "    def setup_logging(self):\n",
        "        \"\"\"Set up logging for the class.\"\"\"\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "        self.logger.setLevel(logging.INFO)\n",
        "\n",
        "        file_handler = logging.FileHandler('enhanced_catboost_predictor.log')\n",
        "        file_handler.setLevel(logging.INFO)\n",
        "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "        file_handler.setFormatter(formatter)\n",
        "        self.logger.addHandler(file_handler)\n",
        "\n",
        "    def engineer_features(self, X, for_training=True):\n",
        "        \"\"\"Create domain-specific engineered features for concrete strength prediction.\n",
        "\n",
        "        Args:\n",
        "            X: Input DataFrame\n",
        "            for_training: If True, calculate and store statistics. If False, use stored statistics.\n",
        "        \"\"\"\n",
        "        # Create a copy of the original dataframe\n",
        "        X_engineered = X.copy()\n",
        "\n",
        "        # Extract component names for readability\n",
        "        cement = X['Cement (component 1)(kg in a m^3 mixture)']\n",
        "        blast_slag = X['Blast Furnace Slag (component 2)(kg in a m^3 mixture)']\n",
        "        fly_ash = X['Fly Ash (component 3)(kg in a m^3 mixture)']\n",
        "        water = X['Water  (component 4)(kg in a m^3 mixture)']\n",
        "        superplast = X['Superplasticizer (component 5)(kg in a m^3 mixture)']\n",
        "        coarse_agg = X['Coarse Aggregate  (component 6)(kg in a m^3 mixture)']\n",
        "        fine_agg = X['Fine Aggregate (component 7)(kg in a m^3 mixture)']\n",
        "        age = X['Age (day)']\n",
        "\n",
        "        # 1. Key concrete engineering ratios\n",
        "        X_engineered['water_cement_ratio'] = water / (cement + 1e-5)\n",
        "        X_engineered['total_cementitious'] = cement + blast_slag + fly_ash\n",
        "        X_engineered['water_cementitious_ratio'] = water / (X_engineered['total_cementitious'] + 1e-5)\n",
        "        X_engineered['agg_cement_ratio'] = (coarse_agg + fine_agg) / (cement + 1e-5)\n",
        "        X_engineered['fine_coarse_ratio'] = fine_agg / (coarse_agg + 1e-5)\n",
        "\n",
        "        # 2. Advanced cement chemistry features\n",
        "        X_engineered['cementitious_superplast_ratio'] = X_engineered['total_cementitious'] / (superplast + 1e-5)\n",
        "        X_engineered['cement_binder_ratio'] = cement / (X_engineered['total_cementitious'] + 1e-5)\n",
        "\n",
        "        # 3. Time-dependent features\n",
        "        X_engineered['log_age'] = np.log1p(age)\n",
        "        X_engineered['sqrt_age'] = np.sqrt(age)\n",
        "        X_engineered['age_28d_ratio'] = age / 28.0  # Normalization by standard 28-day strength\n",
        "\n",
        "        # 4. Physical parameter approximations\n",
        "        X_engineered['paste_volume'] = (cement / 3.15 + blast_slag / 2.9 + fly_ash / 2.3 + water) / \\\n",
        "                                      ((cement / 3.15 + blast_slag / 2.9 + fly_ash / 2.3 + water +\n",
        "                                      coarse_agg / 2.7 + fine_agg / 2.6) + 1e-5)\n",
        "\n",
        "        # 5. Practical concrete mix indicators\n",
        "        X_engineered['slump_indicator'] = water + 10 * superplast\n",
        "        X_engineered['flow_indicator'] = X_engineered['slump_indicator'] / X_engineered['total_cementitious']\n",
        "\n",
        "        # 6. Concrete maturity index\n",
        "        X_engineered['maturity_index'] = age * (1 - np.exp(-0.05 * age))\n",
        "\n",
        "        # 7. Supplementary material utilization\n",
        "        X_engineered['supplementary_fraction'] = (blast_slag + fly_ash) / (X_engineered['total_cementitious'] + 1e-5)\n",
        "\n",
        "        # Enhanced age-related features\n",
        "        X_engineered['early_age_factor'] = np.where(X_engineered['Age (day)'] < 7,\n",
        "                                                (7 - X_engineered['Age (day)'])/7, 0)\n",
        "        X_engineered['very_early_strength'] = X_engineered['Age (day)']**0.5 * X_engineered['Cement (component 1)(kg in a m^3 mixture)']\n",
        "\n",
        "        # Early hydration rate approximation\n",
        "        X_engineered['early_hydration_rate'] = np.where(\n",
        "            X_engineered['Age (day)'] < 7,\n",
        "            X_engineered['Cement (component 1)(kg in a m^3 mixture)'] / (X_engineered['Age (day)'] + 0.5),\n",
        "            0\n",
        "        )\n",
        "\n",
        "        # Late-age strength gain factor\n",
        "        X_engineered['late_age_factor'] = np.where(\n",
        "            X_engineered['Age (day)'] > 28,\n",
        "            np.log1p(X_engineered['Age (day)'] - 28) / 4,\n",
        "            0\n",
        "        )\n",
        "\n",
        "        # CRITICAL PART: Handle statistics properly\n",
        "        if for_training:\n",
        "            # During training: calculate and store statistics\n",
        "            self.feature_stats = {\n",
        "                'total_cementitious_mean': X_engineered['total_cementitious'].mean(),\n",
        "                'total_cementitious_std': X_engineered['total_cementitious'].std(),\n",
        "                'water_cement_ratio_mean': X_engineered['water_cement_ratio'].mean(),\n",
        "                'water_cement_ratio_std': X_engineered['water_cement_ratio'].std(),\n",
        "            }\n",
        "            print(f\"📊 Calculated feature statistics during training: {self.feature_stats}\")\n",
        "\n",
        "            # Use the calculated statistics\n",
        "            total_cem_mean = self.feature_stats['total_cementitious_mean']\n",
        "            water_cem_ratio_mean = self.feature_stats['water_cement_ratio_mean']\n",
        "            water_cem_ratio_std = self.feature_stats['water_cement_ratio_std']\n",
        "        else:\n",
        "            # During prediction: use stored statistics\n",
        "            if not hasattr(self, 'feature_stats') or not self.feature_stats:\n",
        "                raise ValueError(\"Feature statistics not found. Model may not have been trained properly.\")\n",
        "\n",
        "            total_cem_mean = self.feature_stats['total_cementitious_mean']\n",
        "            water_cem_ratio_mean = self.feature_stats['water_cement_ratio_mean']\n",
        "            water_cem_ratio_std = self.feature_stats['water_cement_ratio_std']\n",
        "            print(f\"📊 Using stored feature statistics: {self.feature_stats}\")\n",
        "\n",
        "        # Apply corrections using the statistics\n",
        "        X_engineered['very_low_correction'] = np.where(\n",
        "            X_engineered['total_cementitious'] < total_cem_mean,\n",
        "            -0.05 * X_engineered['water_cementitious_ratio'],\n",
        "            0\n",
        "        )\n",
        "\n",
        "        X_engineered['high_correction'] = np.where(\n",
        "            X_engineered['total_cementitious'] > total_cem_mean * 1.2,\n",
        "            0.05 * X_engineered['cement_binder_ratio'],\n",
        "            0\n",
        "        )\n",
        "\n",
        "        # Feature to detect abnormal mix designs\n",
        "        if water_cem_ratio_std > 0:\n",
        "            X_engineered['abnormal_mix_factor'] = np.abs(\n",
        "                (X_engineered['water_cement_ratio'] - water_cem_ratio_mean) /\n",
        "                water_cem_ratio_std\n",
        "            )\n",
        "        else:\n",
        "            X_engineered['abnormal_mix_factor'] = 0\n",
        "\n",
        "        # Specialized feature for medium strength correction\n",
        "        X_engineered['medium_correction'] = np.where(\n",
        "            (X_engineered['total_cementitious'] >= 350) &\n",
        "            (X_engineered['total_cementitious'] <= 450) &\n",
        "            (X_engineered['water_cement_ratio'] <= 0.5),\n",
        "            -0.1 * X_engineered['total_cementitious'],\n",
        "            0\n",
        "        )\n",
        "\n",
        "        # Feature for very low strength concrete with high water content\n",
        "        X_engineered['water_excess_indicator'] = np.where(\n",
        "            X_engineered['water_cement_ratio'] > 0.6,\n",
        "            X_engineered['water_cement_ratio'] - 0.6,\n",
        "            0\n",
        "        )\n",
        "\n",
        "        # Store feature information (only during training)\n",
        "        if for_training:\n",
        "            self.original_features = X.columns.tolist()\n",
        "            self.engineered_features = [col for col in X_engineered.columns if col not in self.original_features]\n",
        "\n",
        "        return X_engineered\n",
        "\n",
        "\n",
        "    def load_and_preprocess(self, filepath):\n",
        "        \"\"\"Load data and preprocess with enhanced feature engineering.\"\"\"\n",
        "        try:\n",
        "            self.data = pd.read_excel(filepath)\n",
        "            self.logger.info(\"Data loaded successfully\")\n",
        "\n",
        "            # Split features and target\n",
        "            X = self.data.drop(columns=['Concrete compressive strength(MPa, megapascals) '])\n",
        "            y = self.data['Concrete compressive strength(MPa, megapascals) ']\n",
        "\n",
        "            # Create engineered features\n",
        "            X_engineered = self.engineer_features(X, for_training=True)\n",
        "            self.logger.info(f\"Created {len(self.engineered_features)} new engineered features\")\n",
        "\n",
        "            # Create strength ranges for stratified sampling and range-specific models\n",
        "            strength_bins = [0, 20, 40, 60, 100]\n",
        "            strength_labels = ['very_low', 'low', 'medium', 'high']\n",
        "            y_ranges = pd.cut(y, bins=strength_bins, labels=strength_labels)\n",
        "            self.y_ranges = y_ranges\n",
        "            self.strength_bins = strength_bins\n",
        "            self.strength_labels = strength_labels\n",
        "\n",
        "            # Scale features\n",
        "            self.scaler = StandardScaler()\n",
        "            X_scaled = pd.DataFrame(\n",
        "                self.scaler.fit_transform(X_engineered),\n",
        "                columns=X_engineered.columns\n",
        "            )\n",
        "            X_scaled = X_scaled.reset_index(drop=True)\n",
        "\n",
        "            # Store all features\n",
        "            self.all_features = X_scaled.columns.tolist()\n",
        "\n",
        "            # Split data with stratification by strength ranges\n",
        "            X_train, X_test, y_train, y_test, y_ranges_train, y_ranges_test = train_test_split(\n",
        "                X_scaled, y, y_ranges,\n",
        "                test_size=0.2,\n",
        "                random_state=self.random_state,\n",
        "                stratify=y_ranges\n",
        "            )\n",
        "            X_train = X_train.reset_index(drop=True)\n",
        "            X_test = X_test.reset_index(drop=True)\n",
        "\n",
        "            self.X_train = X_train\n",
        "            self.X_test = X_test\n",
        "            self.y_train = y_train\n",
        "            self.y_test = y_test\n",
        "            self.y_ranges_train = y_ranges_train\n",
        "            self.y_ranges_test = y_ranges_test\n",
        "\n",
        "            print(f\"Data split: {X_train.shape} training, {X_test.shape} testing\")\n",
        "            print(\"\\nStrength range distribution in test set:\")\n",
        "            for label in strength_labels:\n",
        "                count = np.sum(y_ranges_test == label)\n",
        "                pct = count / len(y_ranges_test) * 100\n",
        "                print(f\"  {label.replace('_', ' ').title()}: {count} samples ({pct:.1f}%)\")\n",
        "\n",
        "            return X_train, X_test, y_train, y_test, y_ranges_train, y_ranges_test\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error in preprocessing: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def train_deep_catboost(self):\n",
        "        \"\"\"Train a deeper CatBoost model with optimized parameters.\"\"\"\n",
        "        try:\n",
        "            from catboost import CatBoostRegressor, Pool\n",
        "            print(\"\\nTraining deep CatBoost model...\")\n",
        "\n",
        "            # Create CatBoost model with deeper architecture\n",
        "            deep_catboost = CatBoostRegressor(\n",
        "                iterations=2000,          # Increased iterations\n",
        "                learning_rate=0.02,       # Reduced learning rate\n",
        "                depth=8,                  # Increased depth\n",
        "                l2_leaf_reg=3,\n",
        "                loss_function='RMSE',\n",
        "                eval_metric='RMSE',\n",
        "                random_seed=self.random_state,\n",
        "                od_type='Iter',\n",
        "                od_wait=100,              # More patience\n",
        "                verbose=100,\n",
        "                task_type='CPU',          # Use 'GPU' if available\n",
        "                # Advanced parameters\n",
        "                bootstrap_type='Bayesian',\n",
        "                bagging_temperature=1,\n",
        "                grow_policy='SymmetricTree',\n",
        "                min_data_in_leaf=5\n",
        "            )\n",
        "\n",
        "            # Create train and eval pools\n",
        "            train_pool = Pool(self.X_train, self.y_train)\n",
        "            eval_pool = Pool(self.X_test, self.y_test)\n",
        "\n",
        "            # Train model\n",
        "            deep_catboost.fit(\n",
        "                train_pool,\n",
        "                eval_set=eval_pool,\n",
        "                use_best_model=True,\n",
        "                verbose=100\n",
        "            )\n",
        "\n",
        "            # Make predictions\n",
        "            y_pred = deep_catboost.predict(self.X_test)\n",
        "\n",
        "            # Calculate metrics\n",
        "            metrics = self._calculate_metrics(self.y_test, y_pred)\n",
        "            print(\"\\nDeep CatBoost Model Metrics:\")\n",
        "            for metric, value in metrics.items():\n",
        "                print(f\"  {metric}: {value}\")\n",
        "\n",
        "            # Feature importance\n",
        "            importance = deep_catboost.get_feature_importance()\n",
        "            feature_importance = pd.DataFrame({\n",
        "                'Feature': self.X_train.columns,\n",
        "                'Importance': importance\n",
        "            }).sort_values('Importance', ascending=False)\n",
        "\n",
        "            print(\"\\nTop 10 Features by Importance:\")\n",
        "            for idx, row in feature_importance.head(10).iterrows():\n",
        "                print(f\"  {row['Feature']}: {row['Importance']}\")\n",
        "\n",
        "            self.deep_catboost = deep_catboost\n",
        "            self.catboost_feature_importance = feature_importance\n",
        "            self.catboost_metrics = metrics\n",
        "            self.catboost_preds = y_pred\n",
        "\n",
        "            return metrics, y_pred\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"CatBoost is not installed. Please install it using: pip install catboost\")\n",
        "            return None, None\n",
        "\n",
        "    def train_range_specific_models(self):\n",
        "        \"\"\"Train separate models for different concrete strength ranges.\"\"\"\n",
        "        try:\n",
        "            from catboost import CatBoostRegressor, Pool\n",
        "            print(\"\\nTraining strength range-specific models...\")\n",
        "\n",
        "            self.range_models = {}\n",
        "            self.range_preds = {}\n",
        "\n",
        "            # Updated parameters for different ranges with more focus on problematic ranges\n",
        "            range_params = {\n",
        "                'very_low': {  # Less than 20 MPa - Highest error rate\n",
        "                    'iterations': 2000,        # Increased from 1000\n",
        "                    'depth': 7,                # Increased from 6\n",
        "                    'learning_rate': 0.02,     # Lower for more stability\n",
        "                    'l2_leaf_reg': 5,          # Increased regularization\n",
        "                    'bootstrap_type': 'Bayesian',\n",
        "                    'min_data_in_leaf': 5,     # Increased to prevent overfitting\n",
        "                    'random_strength': 0.9     # Increased randomization\n",
        "                },\n",
        "                'low': {  # 20-40 MPa\n",
        "                    'iterations': 1500,\n",
        "                    'depth': 7,\n",
        "                    'learning_rate': 0.02,\n",
        "                    'l2_leaf_reg': 3,\n",
        "                    'bootstrap_type': 'Bayesian'\n",
        "                },\n",
        "                'medium': {  # 40-60 MPa\n",
        "                    'iterations': 1500,\n",
        "                    'depth': 8,\n",
        "                    'learning_rate': 0.02,\n",
        "                    'l2_leaf_reg': 3\n",
        "                },\n",
        "                'high': {  # Over 60 MPa - Few samples but high error rate\n",
        "                    'iterations': 1200,        # Increased from 1000\n",
        "                    'depth': 7,                # Increased from 6\n",
        "                    'learning_rate': 0.015,    # Lower for stability\n",
        "                    'l2_leaf_reg': 4,\n",
        "                    'bootstrap_type': 'Bayesian',\n",
        "                    'bagging_temperature': 1.5 # More aggressive bagging for few samples\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # Train separate model for each strength range\n",
        "            for strength_range in self.strength_labels:\n",
        "                print(f\"\\nTraining model for {strength_range.replace('_', ' ').title()} Strength range...\")\n",
        "\n",
        "                # Make sure indices are aligned properly - convert to numpy arrays if needed\n",
        "                y_ranges_train_array = np.array(self.y_ranges_train)\n",
        "                train_mask = (y_ranges_train_array == strength_range)\n",
        "\n",
        "                # Check if we have enough samples\n",
        "                if np.sum(train_mask) < 10:\n",
        "                    print(f\"  Not enough samples for {strength_range} range. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # Use .loc with indices to avoid alignment issues\n",
        "                train_indices = np.where(train_mask)[0]\n",
        "                X_train_range = self.X_train.iloc[train_indices]\n",
        "                y_train_range = self.y_train.iloc[train_indices]\n",
        "\n",
        "                # Similarly for test data\n",
        "                y_ranges_test_array = np.array(self.y_ranges_test)\n",
        "                test_mask = (y_ranges_test_array == strength_range)\n",
        "                test_indices = np.where(test_mask)[0]\n",
        "\n",
        "                if len(test_indices) < 5:\n",
        "                    print(f\"  Not enough test samples for {strength_range} range. Skipping metrics calculation.\")\n",
        "                    test_samples = 0\n",
        "                else:\n",
        "                    X_test_range = self.X_test.iloc[test_indices]\n",
        "                    y_test_range = self.y_test.iloc[test_indices]\n",
        "                    test_samples = len(X_test_range)\n",
        "\n",
        "                print(f\"  Training samples: {len(X_train_range)}, Test samples: {test_samples}\")\n",
        "\n",
        "                # Get parameters for this range\n",
        "                model_params = range_params.get(strength_range, range_params['low'])  # Default to low params if not found\n",
        "\n",
        "                # Create and train model with range-specific parameters\n",
        "                range_model = CatBoostRegressor(\n",
        "                    iterations=model_params['iterations'],\n",
        "                    learning_rate=model_params['learning_rate'],\n",
        "                    depth=model_params['depth'],\n",
        "                    l2_leaf_reg=model_params.get('l2_leaf_reg', 3),\n",
        "                    loss_function='RMSE',\n",
        "                    eval_metric='RMSE',\n",
        "                    random_seed=self.random_state,\n",
        "                    od_type='Iter',\n",
        "                    od_wait=50,\n",
        "                    verbose=100,\n",
        "                    bootstrap_type=model_params.get('bootstrap_type', 'Bayesian'),\n",
        "                    min_data_in_leaf=model_params.get('min_data_in_leaf', 5),\n",
        "                    random_strength=model_params.get('random_strength', 0.5),\n",
        "                    bagging_temperature=model_params.get('bagging_temperature', 1.0)\n",
        "                )\n",
        "\n",
        "                # Create train pool\n",
        "                train_pool = Pool(X_train_range, y_train_range)\n",
        "\n",
        "                # Create eval pool if we have enough test samples\n",
        "                if test_samples >= 5:\n",
        "                    eval_pool = Pool(X_test_range, y_test_range)\n",
        "\n",
        "                    # Train model with eval set\n",
        "                    range_model.fit(\n",
        "                        train_pool,\n",
        "                        eval_set=eval_pool,\n",
        "                        use_best_model=True,\n",
        "                        verbose=100\n",
        "                    )\n",
        "\n",
        "                    # Calculate metrics\n",
        "                    y_pred_range = range_model.predict(X_test_range)\n",
        "                    metrics = self._calculate_metrics(y_test_range, y_pred_range)\n",
        "\n",
        "                    print(f\"  {strength_range.replace('_', ' ').title()} Range Model Metrics:\")\n",
        "                    for metric, value in metrics.items():\n",
        "                        print(f\"    {metric}: {value}\")\n",
        "                else:\n",
        "                    # Train model without eval set\n",
        "                    range_model.fit(\n",
        "                        train_pool,\n",
        "                        verbose=100\n",
        "                    )\n",
        "\n",
        "                # Store model\n",
        "                self.range_models[strength_range] = range_model\n",
        "\n",
        "                # Make predictions on full test set (for blending later)\n",
        "                self.range_preds[strength_range] = range_model.predict(self.X_test)\n",
        "\n",
        "            return self.range_models, self.range_preds\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in training range-specific models: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None, None\n",
        "\n",
        "    def train_very_low_specialized_models(self):\n",
        "        \"\"\"Train ultra-specialized models for very low strength concrete.\"\"\"\n",
        "        try:\n",
        "            from catboost import CatBoostRegressor, Pool\n",
        "            print(\"\\nTraining specialized models for very low strength concrete...\")\n",
        "\n",
        "            # Get only very low samples using numpy arrays to avoid indexing issues\n",
        "            y_ranges_train_array = np.array(self.y_ranges_train)\n",
        "            mask = (y_ranges_train_array == 'very_low')\n",
        "\n",
        "            # Check if we have enough samples\n",
        "            if np.sum(mask) < 10:\n",
        "                print(\"  Not enough very low strength samples. Skipping.\")\n",
        "                return {}, {}\n",
        "\n",
        "            # Use indices instead of boolean masks\n",
        "            train_indices = np.where(mask)[0]\n",
        "            X_very_low = self.X_train.iloc[train_indices]\n",
        "            y_very_low = self.y_train.iloc[train_indices]\n",
        "\n",
        "            # Further split by actual strength for more specialization\n",
        "            y_very_low_array = np.array(y_very_low)\n",
        "            low_mask = y_very_low_array < 15  # Ultra-low strength\n",
        "            mid_mask = (y_very_low_array >= 15) & (y_very_low_array < 20)  # Mid-low strength\n",
        "\n",
        "            self.very_low_specialized_models = {}\n",
        "            self.very_low_specialized_preds = {}\n",
        "\n",
        "            # Ultra-low strength model\n",
        "            if np.sum(low_mask) >= 10:\n",
        "                # Get indices for the ultra-low samples\n",
        "                ultra_low_indices = np.where(low_mask)[0]\n",
        "\n",
        "                print(f\"  Training ultra-low strength model (<15 MPa) with {len(ultra_low_indices)} samples\")\n",
        "                ultra_low_model = CatBoostRegressor(\n",
        "                    iterations=1500,\n",
        "                    depth=5,  # Lower depth to prevent overfitting on small samples\n",
        "                    learning_rate=0.01,  # Lower learning rate for stability\n",
        "                    l2_leaf_reg=6,  # Higher regularization\n",
        "                    min_data_in_leaf=3,\n",
        "                    verbose=0,\n",
        "                    random_seed=self.random_state\n",
        "                )\n",
        "\n",
        "                # Select rows using iloc with indices\n",
        "                X_ultra_low = X_very_low.iloc[ultra_low_indices]\n",
        "                y_ultra_low = y_very_low.iloc[ultra_low_indices]\n",
        "\n",
        "                ultra_low_model.fit(X_ultra_low, y_ultra_low)\n",
        "                self.very_low_specialized_models['ultra_low'] = ultra_low_model\n",
        "\n",
        "                # Make predictions on test set\n",
        "                self.very_low_specialized_preds['ultra_low'] = np.zeros(len(self.X_test))\n",
        "\n",
        "                # Identify test samples that would use this model\n",
        "                # - First get very_low test samples\n",
        "                y_ranges_test_array = np.array(self.y_ranges_test)\n",
        "                test_mask = (y_ranges_test_array == 'very_low')\n",
        "                test_indices = np.where(test_mask)[0]\n",
        "\n",
        "                # - Then identify which ones are <15 MPa\n",
        "                deep_preds = self.deep_catboost.predict(self.X_test)\n",
        "                ultra_low_test_mask = (deep_preds < 15)\n",
        "\n",
        "                # - Find intersection of very_low and <15 MPa\n",
        "                X_test_very_low = self.X_test.iloc[test_indices]\n",
        "                deep_preds_very_low = deep_preds[test_indices]\n",
        "                ultra_low_test_indices = np.where(deep_preds_very_low < 15)[0]\n",
        "\n",
        "                if len(ultra_low_test_indices) > 0:\n",
        "                    # Calculate metrics\n",
        "                    X_test_ultra_low = X_test_very_low.iloc[ultra_low_test_indices]\n",
        "                    y_test_ultra_low = self.y_test.iloc[test_indices].iloc[ultra_low_test_indices]\n",
        "\n",
        "                    ultra_low_preds = ultra_low_model.predict(X_test_ultra_low)\n",
        "                    metrics = self._calculate_metrics(y_test_ultra_low, ultra_low_preds)\n",
        "\n",
        "                    print(f\"  Ultra-Low Strength Model Metrics (test samples: {len(ultra_low_test_indices)}):\")\n",
        "                    for metric, value in metrics.items():\n",
        "                        print(f\"    {metric}: {value}\")\n",
        "\n",
        "                    # Store predictions for meta-learner - using all test indices\n",
        "                    for idx, very_low_idx in enumerate(test_indices):\n",
        "                        if idx in ultra_low_test_indices:\n",
        "                            test_sample = self.X_test.iloc[[very_low_idx]]\n",
        "                            self.very_low_specialized_preds['ultra_low'][very_low_idx] = ultra_low_model.predict(test_sample)[0]\n",
        "\n",
        "            # Mid-low strength model\n",
        "            if np.sum(mid_mask) >= 10:\n",
        "                # Get indices for the mid-low samples\n",
        "                mid_low_indices = np.where(mid_mask)[0]\n",
        "\n",
        "                print(f\"  Training mid-low strength model (15-20 MPa) with {len(mid_low_indices)} samples\")\n",
        "                mid_low_model = CatBoostRegressor(\n",
        "                    iterations=1500,\n",
        "                    depth=6,\n",
        "                    learning_rate=0.015,\n",
        "                    l2_leaf_reg=4,\n",
        "                    min_data_in_leaf=3,\n",
        "                    verbose=0,\n",
        "                    random_seed=self.random_state\n",
        "                )\n",
        "\n",
        "                # Select rows using iloc with indices\n",
        "                X_mid_low = X_very_low.iloc[mid_low_indices]\n",
        "                y_mid_low = y_very_low.iloc[mid_low_indices]\n",
        "\n",
        "                mid_low_model.fit(X_mid_low, y_mid_low)\n",
        "                self.very_low_specialized_models['mid_low'] = mid_low_model\n",
        "\n",
        "                # Make predictions on test set\n",
        "                self.very_low_specialized_preds['mid_low'] = np.zeros(len(self.X_test))\n",
        "\n",
        "                # Identify test samples that would use this model\n",
        "                # - First get very_low test samples\n",
        "                y_ranges_test_array = np.array(self.y_ranges_test)\n",
        "                test_mask = (y_ranges_test_array == 'very_low')\n",
        "                test_indices = np.where(test_mask)[0]\n",
        "\n",
        "                # - Then identify which ones are 15-20 MPa\n",
        "                deep_preds = self.deep_catboost.predict(self.X_test)\n",
        "\n",
        "                # - Find intersection of very_low and 15-20 MPa\n",
        "                X_test_very_low = self.X_test.iloc[test_indices]\n",
        "                deep_preds_very_low = deep_preds[test_indices]\n",
        "                mid_low_test_indices = np.where((deep_preds_very_low >= 15) & (deep_preds_very_low < 20))[0]\n",
        "\n",
        "                if len(mid_low_test_indices) > 0:\n",
        "                    # Calculate metrics\n",
        "                    X_test_mid_low = X_test_very_low.iloc[mid_low_test_indices]\n",
        "                    y_test_mid_low = self.y_test.iloc[test_indices].iloc[mid_low_test_indices]\n",
        "\n",
        "                    mid_low_preds = mid_low_model.predict(X_test_mid_low)\n",
        "                    metrics = self._calculate_metrics(y_test_mid_low, mid_low_preds)\n",
        "\n",
        "                    print(f\"  Mid-Low Strength Model Metrics (test samples: {len(mid_low_test_indices)}):\")\n",
        "                    for metric, value in metrics.items():\n",
        "                        print(f\"    {metric}: {value}\")\n",
        "\n",
        "                    # Store predictions for meta-learner - using all test indices\n",
        "                    for idx, very_low_idx in enumerate(test_indices):\n",
        "                        if idx in mid_low_test_indices:\n",
        "                            test_sample = self.X_test.iloc[[very_low_idx]]\n",
        "                            self.very_low_specialized_preds['mid_low'][very_low_idx] = mid_low_model.predict(test_sample)[0]\n",
        "\n",
        "            return self.very_low_specialized_models, self.very_low_specialized_preds\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in training very low specialized models: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return {}, {}\n",
        "\n",
        "    def train_medium_bias_correction(self):\n",
        "        \"\"\"Create a bias correction model specifically for medium range.\"\"\"\n",
        "        try:\n",
        "            from catboost import CatBoostRegressor\n",
        "            print(\"\\nTraining medium range bias correction model...\")\n",
        "\n",
        "            # Identify medium range samples using numpy arrays\n",
        "            y_ranges_train_array = np.array(self.y_ranges_train)\n",
        "            mask = (y_ranges_train_array == 'medium')\n",
        "\n",
        "            # Get indices from mask\n",
        "            train_indices = np.where(mask)[0]\n",
        "\n",
        "            if len(train_indices) < 20:\n",
        "                print(\"  Not enough medium range samples for bias correction. Skipping.\")\n",
        "                return None, None\n",
        "\n",
        "            # Use indices to select rows\n",
        "            X_medium = self.X_train.iloc[train_indices]\n",
        "            y_medium = self.y_train.iloc[train_indices]\n",
        "\n",
        "            # Calculate how much our main model over-predicts\n",
        "            main_preds = self.deep_catboost.predict(X_medium)\n",
        "            bias = main_preds - y_medium\n",
        "\n",
        "            print(f\"  Average bias in medium range: {bias.mean():.2f} MPa\")\n",
        "            print(f\"  Max bias in medium range: {bias.max():.2f} MPa\")\n",
        "\n",
        "            # Train a model to predict this bias\n",
        "            bias_model = CatBoostRegressor(\n",
        "                iterations=800,\n",
        "                depth=4,\n",
        "                learning_rate=0.01,\n",
        "                l2_leaf_reg=5,\n",
        "                verbose=0,\n",
        "                random_seed=self.random_state\n",
        "            )\n",
        "\n",
        "            bias_model.fit(X_medium, bias)\n",
        "            self.medium_bias_model = bias_model\n",
        "\n",
        "            # Make predictions on medium range test samples\n",
        "            y_ranges_test_array = np.array(self.y_ranges_test)\n",
        "            medium_test_mask = (y_ranges_test_array == 'medium')\n",
        "            test_indices = np.where(medium_test_mask)[0]\n",
        "\n",
        "            if len(test_indices) > 0:\n",
        "                X_test_medium = self.X_test.iloc[test_indices]\n",
        "                y_test_medium = self.y_test.iloc[test_indices]\n",
        "\n",
        "                # Get the deep model predictions\n",
        "                deep_preds_medium = self.deep_catboost.predict(X_test_medium)\n",
        "\n",
        "                # Get the estimated bias\n",
        "                estimated_bias = self.medium_bias_model.predict(X_test_medium)\n",
        "\n",
        "                # Apply bias correction\n",
        "                corrected_preds = deep_preds_medium - estimated_bias * 0.7  # 70% of the bias\n",
        "\n",
        "                # Calculate metrics\n",
        "                uncorrected_metrics = self._calculate_metrics(y_test_medium, deep_preds_medium)\n",
        "                corrected_metrics = self._calculate_metrics(y_test_medium, corrected_preds)\n",
        "\n",
        "                print(\"\\n  Medium Range Before Correction:\")\n",
        "                for metric, value in uncorrected_metrics.items():\n",
        "                    print(f\"    {metric}: {value}\")\n",
        "\n",
        "                print(\"\\n  Medium Range After Correction:\")\n",
        "                for metric, value in corrected_metrics.items():\n",
        "                    print(f\"    {metric}: {value}\")\n",
        "\n",
        "                # Store the bias predictions for meta-learner\n",
        "                self.medium_bias_preds = np.zeros(len(self.X_test))\n",
        "                for i, idx in enumerate(test_indices):\n",
        "                    self.medium_bias_preds[idx] = estimated_bias[i]\n",
        "\n",
        "            return self.medium_bias_model, getattr(self, 'medium_bias_preds', None)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in training medium bias correction: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None, None\n",
        "\n",
        "    def train_boundary_models(self):\n",
        "        \"\"\"Train specialized models for boundary regions between strength ranges.\"\"\"\n",
        "        try:\n",
        "            from catboost import CatBoostRegressor, Pool\n",
        "            print(\"\\nTraining boundary region models...\")\n",
        "\n",
        "            self.boundary_models = {}\n",
        "            self.boundary_preds = {}\n",
        "\n",
        "            # Define boundary regions with 2 MPa overlap on each side\n",
        "            boundary_regions = [\n",
        "                (15, 25, 'very_low_low_boundary'),  # Between very_low and low\n",
        "                (38, 42, 'low_medium_boundary'),    # Between low and medium\n",
        "                (58, 62, 'medium_high_boundary')    # Between medium and high\n",
        "            ]\n",
        "\n",
        "            for low_bound, high_bound, name in boundary_regions:\n",
        "                print(f\"\\nTraining model for {name.replace('_', ' ').title()} region...\")\n",
        "\n",
        "                # Use numpy arrays to avoid indexing issues\n",
        "                y_train_array = np.array(self.y_train)\n",
        "                mask = (y_train_array >= low_bound) & (y_train_array <= high_bound)\n",
        "\n",
        "                # Check if we have enough samples\n",
        "                sample_count = np.sum(mask)\n",
        "\n",
        "                if sample_count < 20:  # Skip if too few samples\n",
        "                    print(f\"  Insufficient samples ({sample_count}) for {name}. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # Use indices from the mask - this avoids pandas alignment issues\n",
        "                train_indices = np.where(mask)[0]\n",
        "                X_boundary = self.X_train.iloc[train_indices]\n",
        "                y_boundary = self.y_train.iloc[train_indices]\n",
        "\n",
        "                print(f\"  Training with {len(X_boundary)} boundary samples.\")\n",
        "\n",
        "                # Create boundary-specific model\n",
        "                boundary_model = CatBoostRegressor(\n",
        "                    iterations=1200,\n",
        "                    depth=6,\n",
        "                    learning_rate=0.02,\n",
        "                    l2_leaf_reg=3.5,\n",
        "                    loss_function='RMSE',\n",
        "                    eval_metric='RMSE',\n",
        "                    random_seed=self.random_state,\n",
        "                    od_type='Iter',\n",
        "                    od_wait=50,\n",
        "                    verbose=0\n",
        "                )\n",
        "\n",
        "                # Train model\n",
        "                train_pool = Pool(X_boundary, y_boundary)\n",
        "                boundary_model.fit(train_pool, verbose=100)\n",
        "\n",
        "                # Store model\n",
        "                self.boundary_models[name] = boundary_model\n",
        "\n",
        "                # Make predictions on full test set (for blending later)\n",
        "                self.boundary_preds[name] = boundary_model.predict(self.X_test)\n",
        "\n",
        "                # Calculate metrics for boundary region test samples\n",
        "                y_test_array = np.array(self.y_test)\n",
        "                test_mask = (y_test_array >= low_bound) & (y_test_array <= high_bound)\n",
        "                test_indices = np.where(test_mask)[0]\n",
        "\n",
        "                if len(test_indices) > 0:\n",
        "                    X_test_boundary = self.X_test.iloc[test_indices]\n",
        "                    y_test_boundary = self.y_test.iloc[test_indices]\n",
        "\n",
        "                    boundary_preds = boundary_model.predict(X_test_boundary)\n",
        "                    metrics = self._calculate_metrics(y_test_boundary, boundary_preds)\n",
        "\n",
        "                    print(f\"  {name.replace('_', ' ').title()} Model Metrics:\")\n",
        "                    for metric, value in metrics.items():\n",
        "                        print(f\"    {metric}: {value}\")\n",
        "\n",
        "            return self.boundary_models, self.boundary_preds\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in training boundary models: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None, None\n",
        "\n",
        "    def train_age_specific_models(self):\n",
        "        \"\"\"Train specialized models for different concrete age groups.\"\"\"\n",
        "        try:\n",
        "            from catboost import CatBoostRegressor, Pool\n",
        "            print(\"\\nTraining age-specific models...\")\n",
        "\n",
        "            self.age_models = {}\n",
        "            self.age_preds = {}\n",
        "\n",
        "            # Define age bins and labels\n",
        "            age_bins = [0, 3, 7, 28, 90, float('inf')]\n",
        "            age_labels = ['very_early', 'early', 'standard', 'mature', 'old']\n",
        "\n",
        "            # Create age groups\n",
        "            age_col = 'Age (day)'\n",
        "            X_train_age = np.array(self.X_train[age_col])\n",
        "\n",
        "            for i,age_group in enumerate(age_labels):\n",
        "                if i >= len(age_bins) - 1:\n",
        "                    continue  # Skip if we've reached the end of bins\n",
        "\n",
        "                print(f\"\\nTraining model for {age_group.replace('_', ' ').title()} Age concrete...\")\n",
        "\n",
        "                # Get data for this age group using numpy for mask creation\n",
        "                if i == len(age_bins) - 2:  # Last group\n",
        "                    mask = (X_train_age >= age_bins[i]) & (X_train_age <= age_bins[i+1])\n",
        "                else:\n",
        "                    mask = (X_train_age >= age_bins[i]) & (X_train_age < age_bins[i+1])\n",
        "\n",
        "                # Get indices from mask\n",
        "                train_indices = np.where(mask)[0]\n",
        "                sample_count = len(train_indices)\n",
        "\n",
        "                if sample_count < 20:  # Skip if too few samples\n",
        "                    print(f\"  Insufficient samples ({sample_count}) for {age_group} age. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                # Use indices to select rows\n",
        "                X_age = self.X_train.iloc[train_indices]\n",
        "                y_age = self.y_train.iloc[train_indices]\n",
        "\n",
        "                print(f\"  Training with {len(X_age)} age-specific samples.\")\n",
        "\n",
        "                # Create age-specific model with appropriate parameters\n",
        "                if age_group in ['very_early', 'early']:\n",
        "                    # More careful tuning for early-age concrete\n",
        "                    age_model = CatBoostRegressor(\n",
        "                        iterations=1500,\n",
        "                        depth=6,\n",
        "                        learning_rate=0.02,\n",
        "                        l2_leaf_reg=4,\n",
        "                        loss_function='RMSE',\n",
        "                        eval_metric='RMSE',\n",
        "                        random_seed=self.random_state,\n",
        "                        od_type='Iter',\n",
        "                        od_wait=50,\n",
        "                        verbose=0\n",
        "                    )\n",
        "                else:\n",
        "                    age_model = CatBoostRegressor(\n",
        "                        iterations=1200,\n",
        "                        depth=6,\n",
        "                        learning_rate=0.025,\n",
        "                        l2_leaf_reg=3,\n",
        "                        loss_function='RMSE',\n",
        "                        eval_metric='RMSE',\n",
        "                        random_seed=self.random_state,\n",
        "                        od_type='Iter',\n",
        "                        od_wait=50,\n",
        "                        verbose=0\n",
        "                    )\n",
        "\n",
        "                # Train model\n",
        "                train_pool = Pool(X_age, y_age)\n",
        "                age_model.fit(train_pool, verbose=100)\n",
        "\n",
        "                # Store model\n",
        "                self.age_models[age_group] = age_model\n",
        "\n",
        "                # Make predictions on full test set (for blending later)\n",
        "                self.age_preds[age_group] = age_model.predict(self.X_test)\n",
        "\n",
        "                # Calculate metrics for age group test samples\n",
        "                X_test_age = np.array(self.X_test[age_col])\n",
        "                if i == len(age_bins) - 2:  # Last group\n",
        "                    test_mask = (X_test_age >= age_bins[i]) & (X_test_age <= age_bins[i+1])\n",
        "                else:\n",
        "                    test_mask = (X_test_age >= age_bins[i]) & (X_test_age < age_bins[i+1])\n",
        "\n",
        "                test_indices = np.where(test_mask)[0]\n",
        "\n",
        "                if len(test_indices) > 0:\n",
        "                    X_test_age_subset = self.X_test.iloc[test_indices]\n",
        "                    y_test_age = self.y_test.iloc[test_indices]\n",
        "\n",
        "                    age_preds = age_model.predict(X_test_age_subset)\n",
        "                    metrics = self._calculate_metrics(y_test_age, age_preds)\n",
        "\n",
        "                    print(f\"  {age_group.replace('_', ' ').title()} Age Model Metrics:\")\n",
        "                    for metric, value in metrics.items():\n",
        "                        print(f\"    {metric}: {value}\")\n",
        "\n",
        "            return self.age_models, self.age_preds\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"CatBoost is not installed. Please install it using: pip install catboost\")\n",
        "            return None, None\n",
        "\n",
        "    def train_meta_learner(self):\n",
        "        \"\"\"Train a non-linear meta-learner with all specialized models.\"\"\"\n",
        "        if not hasattr(self, 'deep_catboost'):\n",
        "            print(\"Must train deep_catboost first!\")\n",
        "            return None, None\n",
        "\n",
        "        print(\"\\\\nTraining enhanced non-linear meta-learner ensemble...\")\n",
        "\n",
        "        # --- Step 1: Create meta-features from all model predictions ---\n",
        "        meta_features_list = [self.catboost_preds] # Start with deep model predictions\n",
        "\n",
        "        # Dynamically add predictions from all available trained models\n",
        "        model_sets = {\n",
        "            'range_preds': getattr(self, 'range_preds', {}),\n",
        "            'boundary_preds': getattr(self, 'boundary_preds', {}),\n",
        "            'age_preds': getattr(self, 'age_preds', {}),\n",
        "            'very_low_specialized_preds': getattr(self, 'very_low_specialized_preds', {})\n",
        "        }\n",
        "\n",
        "        for pred_dict in model_sets.values():\n",
        "            for pred_array in pred_dict.values():\n",
        "                meta_features_list.append(pred_array)\n",
        "\n",
        "        if hasattr(self, 'medium_bias_preds') and self.medium_bias_preds is not None:\n",
        "            bias_corrected_preds = self.catboost_preds - self.medium_bias_preds * 0.7\n",
        "            meta_features_list.append(bias_corrected_preds)\n",
        "\n",
        "        meta_features_from_models = np.column_stack(meta_features_list)\n",
        "\n",
        "        # --- Step 2: Create range indicators ---\n",
        "        range_indicators = pd.get_dummies(self.y_ranges_test).values\n",
        "\n",
        "        # --- Step 3: Combine everything into the final feature set ---\n",
        "        # This is the crucial fix: combine all parts before creating the DataFrame\n",
        "        final_meta_features_array = np.column_stack([\n",
        "            meta_features_from_models,\n",
        "            range_indicators,\n",
        "            self.X_test.values  # Add original scaled features\n",
        "        ])\n",
        "\n",
        "        # --- Step 4: Create the DataFrame and feature names for the meta-learner ---\n",
        "        # The names must now reflect this final, complete feature set\n",
        "        model_pred_names = [f\"meta_pred_{i}\" for i in range(meta_features_from_models.shape[1])]\n",
        "        indicator_names = [f\"range_{label}\" for label in self.strength_labels]\n",
        "\n",
        "        # Combine all names\n",
        "        self.meta_feature_names = model_pred_names + indicator_names + self.all_features\n",
        "\n",
        "        meta_features_df = pd.DataFrame(final_meta_features_array, columns=self.meta_feature_names)\n",
        "        print(f\"Meta-features created with shape: {meta_features_df.shape}\")\n",
        "\n",
        "        # --- Step 5: Train the meta-learner on the complete feature set ---\n",
        "        from catboost import CatBoostRegressor\n",
        "        from sklearn.model_selection import train_test_split\n",
        "\n",
        "        meta_catboost = CatBoostRegressor(\n",
        "            iterations=1000, learning_rate=0.015, depth=5,\n",
        "            loss_function='RMSE', random_seed=self.random_state, verbose=0,\n",
        "            l2_leaf_reg=4, bootstrap_type='Bayesian'\n",
        "        )\n",
        "\n",
        "        # Split the complete meta-features DataFrame\n",
        "        meta_X_train, meta_X_val, meta_y_train, meta_y_val = train_test_split(\n",
        "            meta_features_df, self.y_test,\n",
        "            test_size=0.3,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "\n",
        "        meta_catboost.fit(meta_X_train, meta_y_train, eval_set=(meta_X_val, meta_y_val))\n",
        "\n",
        "        # Make final predictions\n",
        "        meta_preds = meta_catboost.predict(meta_features_df)\n",
        "\n",
        "        self.meta_learner = meta_catboost\n",
        "        self.meta_learner_type = 'catboost'\n",
        "        self.meta_preds = meta_preds\n",
        "        self.meta_metrics = self._calculate_metrics(self.y_test, meta_preds)\n",
        "\n",
        "        print(\"\\\\nMeta-Learner Metrics:\")\n",
        "        for metric, value in self.meta_metrics.items():\n",
        "            print(f\"  {metric}: {value}\")\n",
        "\n",
        "        self._create_meta_feature_generator()\n",
        "\n",
        "        return self.meta_metrics, self.meta_preds\n",
        "\n",
        "    def _create_meta_feature_generator(self):\n",
        "        \"\"\"Create a function to generate meta-features for new data.\"\"\"\n",
        "        def generate_meta_features(self, X):\n",
        "          \"\"\"Generate meta-features for new data samples.\"\"\"\n",
        "          meta_features = []\n",
        "\n",
        "          # Deep CatBoost predictions\n",
        "          deep_preds = self.deep_catboost.predict(X)\n",
        "          meta_features.append(deep_preds)\n",
        "\n",
        "          # Range-specific models\n",
        "          for range_name in self.strength_labels:\n",
        "              if hasattr(self, 'range_models') and range_name in self.range_models:\n",
        "                  meta_features.append(self.range_models[range_name].predict(X))\n",
        "\n",
        "          # Boundary models\n",
        "          if hasattr(self, 'boundary_models') and self.boundary_models:\n",
        "              for name, model in self.boundary_models.items():\n",
        "                  meta_features.append(model.predict(X))\n",
        "\n",
        "          # Age-specific models\n",
        "          if hasattr(self, 'age_models') and self.age_models:\n",
        "              for age_group, model in self.age_models.items():\n",
        "                  meta_features.append(model.predict(X))\n",
        "\n",
        "          # Very low models\n",
        "          if hasattr(self, 'very_low_specialized_models') and self.very_low_specialized_models:\n",
        "              for name, model in self.very_low_specialized_models.items():\n",
        "                  meta_features.append(model.predict(X))\n",
        "\n",
        "          # Bias-corrected predictions\n",
        "          if hasattr(self, 'medium_bias_model'):\n",
        "              bias_corrected_preds = deep_preds.copy()\n",
        "              medium_mask = (deep_preds >= 40) & (deep_preds < 60)\n",
        "              if np.any(medium_mask):\n",
        "                  medium_indices = np.where(medium_mask)[0]\n",
        "                  X_medium = X.iloc[medium_indices]\n",
        "                  bias_predictions = self.medium_bias_model.predict(X_medium)\n",
        "                  for idx, i in enumerate(medium_indices):\n",
        "                      bias_corrected_preds[i] -= bias_predictions[idx] * 0.7\n",
        "              meta_features.append(bias_corrected_preds)\n",
        "\n",
        "          # Estimate range and create one-hot\n",
        "          estimated_ranges = pd.cut(deep_preds, bins=self.strength_bins, labels=self.strength_labels)\n",
        "          range_indicators = pd.get_dummies(estimated_ranges).reindex(columns=self.strength_labels, fill_value=0).values\n",
        "\n",
        "          # Stack everything\n",
        "          meta_features_array = np.column_stack(meta_features)\n",
        "          meta_features_array = np.column_stack([meta_features_array, range_indicators, X.values])\n",
        "\n",
        "          # Convert to DataFrame with proper column names\n",
        "          meta_feature_names = [f\"meta_{i}\" for i in range(meta_features_array.shape[1])]\n",
        "          meta_features_df = pd.DataFrame(meta_features_array, columns = self.meta_feature_names)\n",
        "\n",
        "\n",
        "          print(\"✅ Final meta feature shape:\", meta_features_array.shape)\n",
        "          if hasattr(self.meta_learner, 'n_features_in_'):\n",
        "              print(f\"📦 Meta-learner expects: {self.meta_learner.n_features_in_} features\")\n",
        "          elif hasattr(self.meta_learner, 'feature_count_'):\n",
        "              print(f\"📦 Meta-learner expects: {self.meta_learner.feature_count_} features\")\n",
        "\n",
        "          return meta_features_df\n",
        "\n",
        "        self.generate_meta_features = generate_meta_features.__get__(self, self.__class__)\n",
        "\n",
        "    def _calculate_metrics(self, y_true, y_pred):\n",
        "        \"\"\"Calculate comprehensive performance metrics.\"\"\"\n",
        "        # Calculate basic regression metrics\n",
        "        mse = mean_squared_error(y_true, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        mae = mean_absolute_error(y_true, y_pred)\n",
        "        r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "        # Calculate percentage errors\n",
        "        percent_errors = np.abs((y_true - y_pred) / y_true * 100)\n",
        "\n",
        "        return {\n",
        "            'r2': r2,\n",
        "            'rmse': rmse,\n",
        "            'mae': mae,\n",
        "            'max_percent_error': np.max(percent_errors),\n",
        "            'mean_percent_error': np.mean(percent_errors),\n",
        "            'median_percent_error': np.median(percent_errors),\n",
        "            'percent_within_5': np.mean(percent_errors <= 5) * 100,\n",
        "            'percent_within_10': np.mean(percent_errors <= 10) * 100\n",
        "        }\n",
        "\n",
        "    def save_model(self, filepath='models/enhanced_catboost_model.joblib'):\n",
        "        \"\"\"Save the trained models and preprocessing objects.\"\"\"\n",
        "        model_dir = Path('models')\n",
        "        model_dir.mkdir(exist_ok=True)\n",
        "\n",
        "        # Create dictionary with all model components\n",
        "        model_data = {\n",
        "            'deep_catboost': getattr(self, 'deep_catboost', None),\n",
        "            'range_models': getattr(self, 'range_models', {}),\n",
        "            'boundary_models': getattr(self, 'boundary_models', {}),\n",
        "            'age_models': getattr(self, 'age_models', {}),\n",
        "            'very_low_specialized_models': getattr(self, 'very_low_specialized_models', {}),\n",
        "            'medium_bias_model': getattr(self, 'medium_bias_model', None),\n",
        "            'meta_learner': getattr(self, 'meta_learner', None),\n",
        "            'meta_learner_type': getattr(self, 'meta_learner_type', None),\n",
        "            'meta_features_scaler': getattr(self, 'meta_features_scaler', None),\n",
        "            'meta_weights': getattr(self, 'meta_weights', None),\n",
        "            'meta_catboost': getattr(self, 'meta_catboost', None),\n",
        "            'meta_mlp': getattr(self, 'meta_mlp', None),\n",
        "            'meta_feature_names': getattr(self, 'meta_feature_names', None),\n",
        "            'scaler': self.scaler,\n",
        "            'original_features': self.original_features,\n",
        "            'engineered_features': self.engineered_features,\n",
        "            'all_features': self.all_features,\n",
        "            'strength_bins': self.strength_bins,\n",
        "            'strength_labels': self.strength_labels,\n",
        "            'random_state': self.random_state,\n",
        "            'catboost_preds': getattr(self, 'catboost_preds', None),\n",
        "            'meta_preds': getattr(self, 'meta_preds', None),\n",
        "            'meta_metrics': getattr(self, 'meta_metrics', None),\n",
        "            'feature_stats': getattr(self, 'feature_stats', {})  # IMPORTANT: Save feature statistics\n",
        "        }\n",
        "\n",
        "        joblib.dump(model_data, filepath)\n",
        "        print(f\"Enhanced CatBoost models saved to {filepath}\")\n",
        "        print(f\"✅ Feature statistics saved: {model_data.get('feature_stats', {})}\")\n",
        "\n",
        "    @classmethod\n",
        "    def load_model(cls, filepath='models/enhanced_catboost_model.joblib'):\n",
        "        \"\"\"Load a trained model and preprocessing objects.\"\"\"\n",
        "        model_data = joblib.load(filepath)\n",
        "\n",
        "        predictor = cls()\n",
        "\n",
        "        if 'meta_feature_names' in model_data:\n",
        "          predictor.meta_feature_names = model_data['meta_feature_names']\n",
        "\n",
        "        for key, value in model_data.items():\n",
        "            setattr(predictor, key, value)\n",
        "\n",
        "        # Recreate meta-feature generator\n",
        "        if hasattr(predictor, 'meta_learner'):\n",
        "            predictor._create_meta_feature_generator()\n",
        "\n",
        "        return predictor\n",
        "\n",
        "    def detect_and_correct_outliers(self, X, predictions):\n",
        "        \"\"\"Detect and correct likely outlier predictions.\"\"\"\n",
        "        corrected_predictions = predictions.copy()\n",
        "\n",
        "        # Get features that might indicate outlier behavior\n",
        "        if 'water_cement_ratio' in X.columns and 'abnormal_mix_factor' in X.columns:\n",
        "            wcr = X['water_cement_ratio']\n",
        "            abnormal_factor = X['abnormal_mix_factor']\n",
        "\n",
        "            # Identify potential outliers based on extreme ratios and factors\n",
        "            wcr_array = np.array(wcr)\n",
        "            abnormal_factor_array = np.array(abnormal_factor)\n",
        "            wcr_high = wcr_array > np.quantile(wcr_array, 0.95)\n",
        "            wcr_low = wcr_array < np.quantile(wcr_array, 0.05)\n",
        "            abnormal_high = abnormal_factor_array > 2.0\n",
        "\n",
        "            potential_outliers = wcr_high | wcr_low | abnormal_high\n",
        "\n",
        "            # For these potential outliers, use a more conservative prediction\n",
        "            outlier_indices = np.where(potential_outliers)[0]\n",
        "            if len(outlier_indices) > 0:\n",
        "                print(f\"Detected {len(outlier_indices)} potential outlier predictions\")\n",
        "\n",
        "                for i in outlier_indices:\n",
        "                    # Estimate strength range based on predicted value\n",
        "                    pred_value = predictions[i]\n",
        "                    if pred_value < 20:\n",
        "                        strength_range = 'very_low'\n",
        "                    elif pred_value < 40:\n",
        "                        strength_range = 'low'\n",
        "                    elif pred_value < 60:\n",
        "                        strength_range = 'medium'\n",
        "                    else:\n",
        "                        strength_range = 'high'\n",
        "\n",
        "                    # Use range-specific model if available\n",
        "                    if hasattr(self, 'range_models') and strength_range in self.range_models:\n",
        "                        # Use iloc with a list to access a single row as DataFrame\n",
        "                        range_pred = self.range_models[strength_range].predict(X.iloc[[i]])[0]\n",
        "                        # Use a weighted average with more weight on range model\n",
        "                        corrected_predictions[i] = 0.3 * predictions[i] + 0.7 * range_pred\n",
        "                        print(f\"  Outlier at index {i}: Original {predictions[i]:.2f}, Corrected {corrected_predictions[i]:.2f}\")\n",
        "\n",
        "        return corrected_predictions\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        if not hasattr(self, 'meta_learner'):\n",
        "            raise ValueError(\"Meta-learner has not been trained. Call train_meta_learner first.\")\n",
        "\n",
        "        # Preprocess data\n",
        "        if isinstance(X_new, pd.DataFrame):\n",
        "            X_engineered = self.engineer_features(X_new, for_training=False)  # Use stored statistics\n",
        "        else:\n",
        "            # Convert to DataFrame if numpy array\n",
        "            X_new_df = pd.DataFrame(X_new, columns=self.original_features)\n",
        "            X_engineered = self.engineer_features(X_new_df, for_training=False)  # Use stored statistics\n",
        "\n",
        "        # Scale features\n",
        "        X_scaled = self.scaler.transform(X_engineered)\n",
        "        X_scaled_df = pd.DataFrame(X_scaled, columns=self.all_features)\n",
        "\n",
        "        # Generate meta-features\n",
        "        meta_features = self.generate_meta_features(X_scaled_df)\n",
        "\n",
        "        # Make predictions using meta-learner\n",
        "        predictions = self.meta_learner.predict(meta_features)\n",
        "\n",
        "        # Apply outlier detection and correction\n",
        "        predictions = self.detect_and_correct_outliers(X_scaled_df, predictions)\n",
        "\n",
        "        # Apply range-specific corrections\n",
        "        final_predictions = []\n",
        "\n",
        "        for i, pred in enumerate(predictions):\n",
        "            # Determine likely strength range\n",
        "            if pred < 20:\n",
        "                strength_range = 'very_low'\n",
        "            elif pred < 40:\n",
        "                strength_range = 'low'\n",
        "            elif pred < 60:\n",
        "                strength_range = 'medium'\n",
        "            else:\n",
        "                strength_range = 'high'\n",
        "\n",
        "            # Apply specialized corrections\n",
        "            if strength_range == 'very_low':\n",
        "                # Check for specialized very low models\n",
        "                if hasattr(self, 'very_low_specialized_models'):\n",
        "                    if pred < 15 and 'ultra_low' in self.very_low_specialized_models:\n",
        "                        # Get specialized prediction\n",
        "                        specialized_pred = self.very_low_specialized_models['ultra_low'].predict(X_scaled_df.iloc[[i]])[0]\n",
        "                        # Use a weighted blend\n",
        "                        pred = 0.4 * pred + 0.6 * specialized_pred\n",
        "                    elif pred >= 15 and pred < 20 and 'mid_low' in self.very_low_specialized_models:\n",
        "                        specialized_pred = self.very_low_specialized_models['mid_low'].predict(X_scaled_df.iloc[[i]])[0]\n",
        "                        pred = 0.4 * pred + 0.6 * specialized_pred\n",
        "\n",
        "            elif strength_range == 'medium':\n",
        "                # Apply bias correction for medium range\n",
        "                if hasattr(self, 'medium_bias_model'):\n",
        "                    estimated_bias = self.medium_bias_model.predict(X_scaled_df.iloc[[i]])[0]\n",
        "                    # If bias is significant\n",
        "                    if estimated_bias > 5:\n",
        "                        # Reduce the prediction by the estimated bias\n",
        "                        pred -= estimated_bias * 0.7  # Using 70% of the bias as a safe measure\n",
        "\n",
        "            elif strength_range == 'high':\n",
        "                # Boost high strength predictions to address under-prediction\n",
        "                pred *= 1.05  # Apply a 5% boost\n",
        "\n",
        "            final_predictions.append(pred)\n",
        "\n",
        "        return np.array(final_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Initialize the predictor\n",
        "predictor = EnhancedCatBoostPredictor(random_state=42)\n",
        "\n",
        "# 2. Load and preprocess the data\n",
        "print(\"--- Loading and Preprocessing Data ---\")\n",
        "predictor.load_and_preprocess(\"Concrete_Data.xls\")\n",
        "\n",
        "# 3. Train the base model\n",
        "print(\"\\\\n--- Training Base CatBoost Model ---\")\n",
        "predictor.train_deep_catboost()\n",
        "\n",
        "# 4. Train all specialized models\n",
        "print(\"\\\\n--- Training Specialized Models ---\")\n",
        "predictor.train_range_specific_models()\n",
        "predictor.train_boundary_models()\n",
        "predictor.train_age_specific_models()\n",
        "predictor.train_very_low_specialized_models()\n",
        "predictor.train_medium_bias_correction()\n",
        "\n",
        "# 5. Train the final meta-learner\n",
        "print(\"\\\\n--- Training Meta-Learner ---\")\n",
        "predictor.train_meta_learner()\n",
        "\n",
        "# 6. Save the complete, trained model\n",
        "print(\"\\\\n--- Saving Model ---\")\n",
        "predictor.save_model(\"models/enhanced_catboost_model.joblib\")\n",
        "\n",
        "print(\"\\\\n✅ Training pipeline complete. Model is saved and ready for deployment.\")"
      ],
      "metadata": {
        "id": "Z3qW55V58lKF",
        "outputId": "191f065e-f349-4367-e336-ca8cfedb8ab7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:Data loaded successfully\n",
            "INFO:__main__:Created 24 new engineered features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading and Preprocessing Data ---\n",
            "📊 Calculated feature statistics during training: {'total_cementitious_mean': np.float64(409.2482524271844), 'total_cementitious_std': 92.78329015939812, 'water_cement_ratio_mean': np.float64(0.7482685847867987), 'water_cement_ratio_std': 0.3140053883466934}\n",
            "Data split: (824, 32) training, (206, 32) testing\n",
            "\n",
            "Strength range distribution in test set:\n",
            "  Very Low: 39 samples (18.9%)\n",
            "  Low: 91 samples (44.2%)\n",
            "  Medium: 57 samples (27.7%)\n",
            "  High: 19 samples (9.2%)\n",
            "\\n--- Training Base CatBoost Model ---\n",
            "\n",
            "Training deep CatBoost model...\n",
            "0:\tlearn: 16.3773542\ttest: 16.8433682\tbest: 16.8433682 (0)\ttotal: 80.7ms\tremaining: 2m 41s\n",
            "100:\tlearn: 6.1136361\ttest: 6.8700188\tbest: 6.8700188 (100)\ttotal: 3.79s\tremaining: 1m 11s\n",
            "200:\tlearn: 4.2346431\ttest: 5.4830746\tbest: 5.4830746 (200)\ttotal: 7.07s\tremaining: 1m 3s\n",
            "300:\tlearn: 3.5294387\ttest: 5.0404466\tbest: 5.0404466 (300)\ttotal: 10.1s\tremaining: 57.2s\n",
            "400:\tlearn: 3.0696553\ttest: 4.7563153\tbest: 4.7563153 (400)\ttotal: 13.7s\tremaining: 54.5s\n",
            "500:\tlearn: 2.7190146\ttest: 4.5948980\tbest: 4.5948980 (500)\ttotal: 17.9s\tremaining: 53.7s\n",
            "600:\tlearn: 2.4682995\ttest: 4.4794460\tbest: 4.4794460 (600)\ttotal: 21s\tremaining: 48.8s\n",
            "700:\tlearn: 2.2491978\ttest: 4.3924141\tbest: 4.3924141 (700)\ttotal: 22.2s\tremaining: 41.2s\n",
            "800:\tlearn: 2.0811114\ttest: 4.3347376\tbest: 4.3347376 (800)\ttotal: 23.5s\tremaining: 35.1s\n",
            "900:\tlearn: 1.9414535\ttest: 4.2742268\tbest: 4.2742268 (900)\ttotal: 24.8s\tremaining: 30.2s\n",
            "1000:\tlearn: 1.8189255\ttest: 4.2211912\tbest: 4.2211912 (1000)\ttotal: 26.4s\tremaining: 26.3s\n",
            "1100:\tlearn: 1.7155154\ttest: 4.1787658\tbest: 4.1787658 (1100)\ttotal: 28.4s\tremaining: 23.2s\n",
            "1200:\tlearn: 1.6351937\ttest: 4.1467289\tbest: 4.1467289 (1200)\ttotal: 29.7s\tremaining: 19.7s\n",
            "1300:\tlearn: 1.5676324\ttest: 4.1135237\tbest: 4.1135237 (1300)\ttotal: 30.9s\tremaining: 16.6s\n",
            "1400:\tlearn: 1.5113649\ttest: 4.0887888\tbest: 4.0887888 (1400)\ttotal: 32.2s\tremaining: 13.8s\n",
            "1500:\tlearn: 1.4596184\ttest: 4.0678508\tbest: 4.0678508 (1500)\ttotal: 33.5s\tremaining: 11.1s\n",
            "1600:\tlearn: 1.4135254\ttest: 4.0474599\tbest: 4.0474599 (1600)\ttotal: 34.7s\tremaining: 8.66s\n",
            "1700:\tlearn: 1.3726557\ttest: 4.0316852\tbest: 4.0316852 (1700)\ttotal: 36s\tremaining: 6.33s\n",
            "1800:\tlearn: 1.3387501\ttest: 4.0174647\tbest: 4.0174647 (1800)\ttotal: 37.2s\tremaining: 4.11s\n",
            "1900:\tlearn: 1.3045987\ttest: 4.0046379\tbest: 4.0046379 (1900)\ttotal: 39.3s\tremaining: 2.04s\n",
            "1999:\tlearn: 1.2758032\ttest: 3.9879580\tbest: 3.9876280 (1995)\ttotal: 40.9s\tremaining: 0us\n",
            "\n",
            "bestTest = 3.987627961\n",
            "bestIteration = 1995\n",
            "\n",
            "Shrink model to first 1996 iterations.\n",
            "\n",
            "Deep CatBoost Model Metrics:\n",
            "  r2: 0.9454105850797221\n",
            "  rmse: 3.9876280695215987\n",
            "  mae: 2.541734733167335\n",
            "  max_percent_error: 53.14030601586066\n",
            "  mean_percent_error: 8.018560171532583\n",
            "  median_percent_error: 5.397953349717742\n",
            "  percent_within_5: 48.05825242718447\n",
            "  percent_within_10: 74.75728155339806\n",
            "\n",
            "Top 10 Features by Importance:\n",
            "  very_early_strength: 22.02073683389367\n",
            "  water_cementitious_ratio: 13.57919245187319\n",
            "  Blast Furnace Slag (component 2)(kg in a m^3 mixture): 4.188075635394838\n",
            "  Water  (component 4)(kg in a m^3 mixture): 4.0358325160525625\n",
            "  high_correction: 3.375326844976841\n",
            "  total_cementitious: 3.2764559419417614\n",
            "  very_low_correction: 3.2296559747518137\n",
            "  slump_indicator: 3.1854392279986152\n",
            "  maturity_index: 3.1727121195705124\n",
            "  Fly Ash (component 3)(kg in a m^3 mixture): 2.8596398830834056\n",
            "\\n--- Training Specialized Models ---\n",
            "\n",
            "Training strength range-specific models...\n",
            "\n",
            "Training model for Very Low Strength range...\n",
            "  Training samples: 158, Test samples: 39\n",
            "0:\tlearn: 4.0389472\ttest: 3.3239354\tbest: 3.3239354 (0)\ttotal: 3.56ms\tremaining: 7.12s\n",
            "100:\tlearn: 2.5743132\ttest: 2.5735631\tbest: 2.5735631 (100)\ttotal: 285ms\tremaining: 5.36s\n",
            "200:\tlearn: 1.9464592\ttest: 2.2855583\tbest: 2.2855583 (200)\ttotal: 580ms\tremaining: 5.19s\n",
            "300:\tlearn: 1.5616363\ttest: 2.1736765\tbest: 2.1736765 (300)\ttotal: 870ms\tremaining: 4.91s\n",
            "400:\tlearn: 1.2608670\ttest: 2.1383783\tbest: 2.1379128 (394)\ttotal: 1.18s\tremaining: 4.69s\n",
            "500:\tlearn: 1.0307389\ttest: 2.1265864\tbest: 2.1240791 (487)\ttotal: 1.47s\tremaining: 4.41s\n",
            "Stopped by overfitting detector  (50 iterations wait)\n",
            "\n",
            "bestTest = 2.122767836\n",
            "bestIteration = 515\n",
            "\n",
            "Shrink model to first 516 iterations.\n",
            "  Very Low Range Model Metrics:\n",
            "    r2: 0.5868293996228349\n",
            "    rmse: 2.1227678884153742\n",
            "    mae: 1.4854258388119068\n",
            "    max_percent_error: 53.6679890667082\n",
            "    mean_percent_error: 11.728413825661422\n",
            "    median_percent_error: 7.033027240346199\n",
            "    percent_within_5: 41.02564102564102\n",
            "    percent_within_10: 58.97435897435898\n",
            "\n",
            "Training model for Low Strength range...\n",
            "  Training samples: 363, Test samples: 91\n",
            "0:\tlearn: 5.5544982\ttest: 6.0631401\tbest: 6.0631401 (0)\ttotal: 6.56ms\tremaining: 9.83s\n",
            "100:\tlearn: 3.4760665\ttest: 4.3418365\tbest: 4.3418365 (100)\ttotal: 560ms\tremaining: 7.75s\n",
            "200:\tlearn: 2.7222603\ttest: 3.8093688\tbest: 3.8093688 (200)\ttotal: 1.1s\tremaining: 7.12s\n",
            "300:\tlearn: 2.2770635\ttest: 3.5684022\tbest: 3.5675047 (299)\ttotal: 1.71s\tremaining: 6.82s\n",
            "400:\tlearn: 1.9193044\ttest: 3.4080928\tbest: 3.4080928 (400)\ttotal: 2.25s\tremaining: 6.16s\n",
            "500:\tlearn: 1.6358157\ttest: 3.3229972\tbest: 3.3229972 (500)\ttotal: 2.81s\tremaining: 5.61s\n",
            "600:\tlearn: 1.4250289\ttest: 3.2643486\tbest: 3.2630662 (599)\ttotal: 3.38s\tremaining: 5.05s\n",
            "700:\tlearn: 1.2565521\ttest: 3.2183633\tbest: 3.2183633 (700)\ttotal: 3.92s\tremaining: 4.47s\n",
            "800:\tlearn: 1.1161888\ttest: 3.1933825\tbest: 3.1918692 (790)\ttotal: 4.47s\tremaining: 3.9s\n",
            "900:\tlearn: 1.0017840\ttest: 3.1634389\tbest: 3.1634389 (900)\ttotal: 5.02s\tremaining: 3.33s\n",
            "1000:\tlearn: 0.9074687\ttest: 3.1438673\tbest: 3.1438457 (998)\ttotal: 5.58s\tremaining: 2.78s\n",
            "1100:\tlearn: 0.8259285\ttest: 3.1316665\tbest: 3.1306108 (1091)\ttotal: 6.15s\tremaining: 2.23s\n",
            "1200:\tlearn: 0.7542809\ttest: 3.1248681\tbest: 3.1248546 (1195)\ttotal: 6.72s\tremaining: 1.67s\n",
            "1300:\tlearn: 0.6929233\ttest: 3.1152096\tbest: 3.1147575 (1298)\ttotal: 7.35s\tremaining: 1.12s\n",
            "1400:\tlearn: 0.6374285\ttest: 3.1111524\tbest: 3.1098479 (1378)\ttotal: 8.63s\tremaining: 610ms\n",
            "1499:\tlearn: 0.5888845\ttest: 3.1033183\tbest: 3.1033183 (1499)\ttotal: 9.57s\tremaining: 0us\n",
            "\n",
            "bestTest = 3.103318275\n",
            "bestIteration = 1499\n",
            "\n",
            "  Low Range Model Metrics:\n",
            "    r2: 0.7364903647371872\n",
            "    rmse: 3.1033182878324994\n",
            "    mae: 2.280311507937772\n",
            "    max_percent_error: 60.363917847502194\n",
            "    mean_percent_error: 8.02242579339765\n",
            "    median_percent_error: 6.534257022024838\n",
            "    percent_within_5: 42.857142857142854\n",
            "    percent_within_10: 71.42857142857143\n",
            "\n",
            "Training model for Medium Strength range...\n",
            "  Training samples: 228, Test samples: 57\n",
            "0:\tlearn: 5.5189827\ttest: 6.4205650\tbest: 6.4205650 (0)\ttotal: 9.5ms\tremaining: 14.2s\n",
            "100:\tlearn: 3.5170791\ttest: 4.9458181\tbest: 4.9458181 (100)\ttotal: 600ms\tremaining: 8.3s\n",
            "200:\tlearn: 2.6383599\ttest: 4.4886580\tbest: 4.4886580 (200)\ttotal: 1.2s\tremaining: 7.77s\n",
            "300:\tlearn: 2.0915918\ttest: 4.2806517\tbest: 4.2806517 (300)\ttotal: 1.8s\tremaining: 7.17s\n",
            "400:\tlearn: 1.6803764\ttest: 4.1459514\tbest: 4.1458971 (399)\ttotal: 2.43s\tremaining: 6.65s\n",
            "500:\tlearn: 1.3871390\ttest: 4.0800776\tbest: 4.0799001 (498)\ttotal: 3.04s\tremaining: 6.05s\n",
            "600:\tlearn: 1.1661797\ttest: 4.0514193\tbest: 4.0495653 (596)\ttotal: 3.62s\tremaining: 5.41s\n",
            "700:\tlearn: 0.9973136\ttest: 4.0277024\tbest: 4.0257137 (684)\ttotal: 4.23s\tremaining: 4.82s\n",
            "800:\tlearn: 0.8656110\ttest: 4.0086889\tbest: 4.0077848 (795)\ttotal: 4.82s\tremaining: 4.21s\n",
            "900:\tlearn: 0.7636913\ttest: 3.9930631\tbest: 3.9913553 (892)\ttotal: 5.44s\tremaining: 3.62s\n",
            "1000:\tlearn: 0.6804767\ttest: 3.9850005\tbest: 3.9850005 (1000)\ttotal: 6.08s\tremaining: 3.03s\n",
            "Stopped by overfitting detector  (50 iterations wait)\n",
            "\n",
            "bestTest = 3.98500054\n",
            "bestIteration = 1000\n",
            "\n",
            "Shrink model to first 1001 iterations.\n",
            "  Medium Range Model Metrics:\n",
            "    r2: 0.5998880657262784\n",
            "    rmse: 3.9850003243328738\n",
            "    mae: 2.933543189104827\n",
            "    max_percent_error: 25.646612455085126\n",
            "    mean_percent_error: 5.9422651978549865\n",
            "    median_percent_error: 4.034734002401219\n",
            "    percent_within_5: 56.14035087719298\n",
            "    percent_within_10: 80.7017543859649\n",
            "\n",
            "Training model for High Strength range...\n",
            "  Training samples: 75, Test samples: 19\n",
            "0:\tlearn: 6.1138098\ttest: 6.0891569\tbest: 6.0891569 (0)\ttotal: 1.19ms\tremaining: 1.43s\n",
            "100:\tlearn: 4.0314164\ttest: 5.2365626\tbest: 5.2365626 (100)\ttotal: 114ms\tremaining: 1.24s\n",
            "200:\tlearn: 2.9635662\ttest: 5.0530470\tbest: 5.0527545 (196)\ttotal: 235ms\tremaining: 1.17s\n",
            "Stopped by overfitting detector  (50 iterations wait)\n",
            "\n",
            "bestTest = 5.038635428\n",
            "bestIteration = 221\n",
            "\n",
            "Shrink model to first 222 iterations.\n",
            "  High Range Model Metrics:\n",
            "    r2: 0.315254354626057\n",
            "    rmse: 5.038636004347666\n",
            "    mae: 4.252510850466629\n",
            "    max_percent_error: 17.25939297404146\n",
            "    mean_percent_error: 6.2247789522815\n",
            "    median_percent_error: 6.63449440556783\n",
            "    percent_within_5: 36.84210526315789\n",
            "    percent_within_10: 89.47368421052632\n",
            "\n",
            "Training boundary region models...\n",
            "\n",
            "Training model for Very Low Low Boundary region...\n",
            "  Training with 135 boundary samples.\n",
            "0:\tlearn: 3.0899784\ttotal: 2.35ms\tremaining: 2.81s\n",
            "100:\tlearn: 2.0264192\ttotal: 204ms\tremaining: 2.21s\n",
            "200:\tlearn: 1.4816711\ttotal: 379ms\tremaining: 1.88s\n",
            "300:\tlearn: 1.1471572\ttotal: 559ms\tremaining: 1.67s\n",
            "400:\tlearn: 0.8790099\ttotal: 744ms\tremaining: 1.48s\n",
            "500:\tlearn: 0.6930067\ttotal: 929ms\tremaining: 1.29s\n",
            "600:\tlearn: 0.5681879\ttotal: 1.11s\tremaining: 1.11s\n",
            "700:\tlearn: 0.4779416\ttotal: 1.31s\tremaining: 933ms\n",
            "800:\tlearn: 0.4021013\ttotal: 1.49s\tremaining: 741ms\n",
            "900:\tlearn: 0.3368127\ttotal: 1.67s\tremaining: 553ms\n",
            "1000:\tlearn: 0.2873395\ttotal: 1.85s\tremaining: 368ms\n",
            "1100:\tlearn: 0.2450038\ttotal: 2.04s\tremaining: 183ms\n",
            "1199:\tlearn: 0.2093903\ttotal: 2.24s\tremaining: 0us\n",
            "  Very Low Low Boundary Model Metrics:\n",
            "    r2: 0.5555597199966646\n",
            "    rmse: 2.1306094743284287\n",
            "    mae: 1.720238806162473\n",
            "    max_percent_error: 34.22750707196944\n",
            "    mean_percent_error: 9.096752755969101\n",
            "    median_percent_error: 8.507795169645046\n",
            "    percent_within_5: 34.883720930232556\n",
            "    percent_within_10: 60.46511627906976\n",
            "\n",
            "Training model for Low Medium Boundary region...\n",
            "  Training with 86 boundary samples.\n",
            "0:\tlearn: 1.1144166\ttotal: 1.47ms\tremaining: 1.76s\n",
            "100:\tlearn: 0.8111569\ttotal: 125ms\tremaining: 1.35s\n",
            "200:\tlearn: 0.6230948\ttotal: 245ms\tremaining: 1.22s\n",
            "300:\tlearn: 0.4861350\ttotal: 371ms\tremaining: 1.11s\n",
            "400:\tlearn: 0.4073583\ttotal: 533ms\tremaining: 1.06s\n",
            "500:\tlearn: 0.3382889\ttotal: 808ms\tremaining: 1.13s\n",
            "600:\tlearn: 0.2863427\ttotal: 1.22s\tremaining: 1.22s\n",
            "700:\tlearn: 0.2530842\ttotal: 1.52s\tremaining: 1.08s\n",
            "800:\tlearn: 0.2240050\ttotal: 1.93s\tremaining: 963ms\n",
            "900:\tlearn: 0.1985850\ttotal: 2.28s\tremaining: 758ms\n",
            "1000:\tlearn: 0.1747505\ttotal: 2.65s\tremaining: 527ms\n",
            "1100:\tlearn: 0.1564548\ttotal: 2.79s\tremaining: 251ms\n",
            "1199:\tlearn: 0.1431361\ttotal: 2.91s\tremaining: 0us\n",
            "  Low Medium Boundary Model Metrics:\n",
            "    r2: -0.4517056891217457\n",
            "    rmse: 1.2579799547271986\n",
            "    mae: 0.9458787089935751\n",
            "    max_percent_error: 6.809466414459477\n",
            "    mean_percent_error: 2.3627638187912106\n",
            "    median_percent_error: 2.2938240874617533\n",
            "    percent_within_5: 85.0\n",
            "    percent_within_10: 100.0\n",
            "\n",
            "Training model for Medium High Boundary region...\n",
            "  Training with 21 boundary samples.\n",
            "0:\tlearn: 1.0039838\ttotal: 437us\tremaining: 525ms\n",
            "100:\tlearn: 0.6850376\ttotal: 63.6ms\tremaining: 692ms\n",
            "200:\tlearn: 0.4585761\ttotal: 110ms\tremaining: 548ms\n",
            "300:\tlearn: 0.2756679\ttotal: 162ms\tremaining: 485ms\n",
            "400:\tlearn: 0.1597705\ttotal: 210ms\tremaining: 419ms\n",
            "500:\tlearn: 0.0975174\ttotal: 260ms\tremaining: 362ms\n",
            "600:\tlearn: 0.0595108\ttotal: 309ms\tremaining: 308ms\n",
            "700:\tlearn: 0.0378699\ttotal: 363ms\tremaining: 258ms\n",
            "800:\tlearn: 0.0239967\ttotal: 417ms\tremaining: 208ms\n",
            "900:\tlearn: 0.0151828\ttotal: 471ms\tremaining: 156ms\n",
            "1000:\tlearn: 0.0096181\ttotal: 522ms\tremaining: 104ms\n",
            "1100:\tlearn: 0.0060934\ttotal: 572ms\tremaining: 51.4ms\n",
            "1199:\tlearn: 0.0038770\ttotal: 620ms\tremaining: 0us\n",
            "  Medium High Boundary Model Metrics:\n",
            "    r2: -0.5289191159796738\n",
            "    rmse: 1.5279730772041615\n",
            "    mae: 1.2861458114822852\n",
            "    max_percent_error: 5.054440567612098\n",
            "    mean_percent_error: 2.1371968377650234\n",
            "    median_percent_error: 1.7911945035813157\n",
            "    percent_within_5: 88.88888888888889\n",
            "    percent_within_10: 100.0\n",
            "\n",
            "Training age-specific models...\n",
            "\n",
            "Training model for Very Early Age concrete...\n",
            "  Training with 200 age-specific samples.\n",
            "0:\tlearn: 13.5459530\ttotal: 2.92ms\tremaining: 4.38s\n",
            "100:\tlearn: 5.7364047\ttotal: 180ms\tremaining: 2.49s\n",
            "200:\tlearn: 3.7657109\ttotal: 381ms\tremaining: 2.46s\n",
            "300:\tlearn: 2.8688144\ttotal: 560ms\tremaining: 2.23s\n",
            "400:\tlearn: 2.2757596\ttotal: 739ms\tremaining: 2.02s\n",
            "500:\tlearn: 1.8804521\ttotal: 917ms\tremaining: 1.83s\n",
            "600:\tlearn: 1.6114650\ttotal: 1.1s\tremaining: 1.65s\n",
            "700:\tlearn: 1.4408271\ttotal: 1.27s\tremaining: 1.45s\n",
            "800:\tlearn: 1.2914264\ttotal: 1.47s\tremaining: 1.28s\n",
            "900:\tlearn: 1.1647306\ttotal: 1.65s\tremaining: 1.1s\n",
            "1000:\tlearn: 1.0533408\ttotal: 1.82s\tremaining: 910ms\n",
            "1100:\tlearn: 0.9587093\ttotal: 2.01s\tremaining: 729ms\n",
            "1200:\tlearn: 0.8670434\ttotal: 2.19s\tremaining: 545ms\n",
            "1300:\tlearn: 0.7838743\ttotal: 2.39s\tremaining: 365ms\n",
            "1400:\tlearn: 0.7068529\ttotal: 2.57s\tremaining: 181ms\n",
            "1499:\tlearn: 0.6502828\ttotal: 2.75s\tremaining: 0us\n",
            "  Very Early Age Model Metrics:\n",
            "    r2: 0.9398309801305808\n",
            "    rmse: 3.457864361313552\n",
            "    mae: 2.6851837493265016\n",
            "    max_percent_error: 21.697842725780024\n",
            "    mean_percent_error: 5.796975144321247\n",
            "    median_percent_error: 4.343330668364111\n",
            "    percent_within_5: 58.333333333333336\n",
            "    percent_within_10: 83.33333333333334\n",
            "\n",
            "Training model for Early Age concrete...\n",
            "  Training with 27 age-specific samples.\n",
            "0:\tlearn: 9.0858866\ttotal: 486us\tremaining: 729ms\n",
            "100:\tlearn: 5.0052724\ttotal: 37.6ms\tremaining: 520ms\n",
            "200:\tlearn: 3.0757686\ttotal: 75.8ms\tremaining: 490ms\n",
            "300:\tlearn: 2.2413987\ttotal: 118ms\tremaining: 470ms\n",
            "400:\tlearn: 1.5882729\ttotal: 164ms\tremaining: 451ms\n",
            "500:\tlearn: 1.1114728\ttotal: 208ms\tremaining: 415ms\n",
            "600:\tlearn: 0.7834790\ttotal: 255ms\tremaining: 382ms\n",
            "700:\tlearn: 0.5535072\ttotal: 296ms\tremaining: 337ms\n",
            "800:\tlearn: 0.3977558\ttotal: 337ms\tremaining: 294ms\n",
            "900:\tlearn: 0.2764005\ttotal: 379ms\tremaining: 252ms\n",
            "1000:\tlearn: 0.1973450\ttotal: 421ms\tremaining: 210ms\n",
            "1100:\tlearn: 0.1525178\ttotal: 463ms\tremaining: 168ms\n",
            "1200:\tlearn: 0.1191878\ttotal: 501ms\tremaining: 125ms\n",
            "1300:\tlearn: 0.0827506\ttotal: 561ms\tremaining: 85.8ms\n",
            "1400:\tlearn: 0.0618371\ttotal: 603ms\tremaining: 42.6ms\n",
            "1499:\tlearn: 0.0443721\ttotal: 643ms\tremaining: 0us\n",
            "  Early Age Model Metrics:\n",
            "    r2: 0.9173801473423285\n",
            "    rmse: 3.4313380360034165\n",
            "    mae: 2.084678026913881\n",
            "    max_percent_error: 10.63290164653897\n",
            "    mean_percent_error: 3.5356757543623005\n",
            "    median_percent_error: 1.9321243574785298\n",
            "    percent_within_5: 66.66666666666666\n",
            "    percent_within_10: 83.33333333333334\n",
            "\n",
            "Training model for Standard Age concrete...\n",
            "  Insufficient samples (0) for standard age. Skipping.\n",
            "\n",
            "Training model for Mature Age concrete...\n",
            "  Insufficient samples (0) for mature age. Skipping.\n",
            "\n",
            "Training model for Old Age concrete...\n",
            "  Insufficient samples (0) for old age. Skipping.\n",
            "\n",
            "Training specialized models for very low strength concrete...\n",
            "  Training ultra-low strength model (<15 MPa) with 96 samples\n",
            "  Ultra-Low Strength Model Metrics (test samples: 22):\n",
            "    r2: 0.1379019488161557\n",
            "    rmse: 2.860730943051122\n",
            "    mae: 2.2041125185939787\n",
            "    max_percent_error: 50.98246668278353\n",
            "    mean_percent_error: 17.82074012196806\n",
            "    median_percent_error: 13.754353343113694\n",
            "    percent_within_5: 18.181818181818183\n",
            "    percent_within_10: 31.818181818181817\n",
            "  Training mid-low strength model (15-20 MPa) with 62 samples\n",
            "  Mid-Low Strength Model Metrics (test samples: 16):\n",
            "    r2: -0.4874999412554719\n",
            "    rmse: 2.789534969578861\n",
            "    mae: 1.9031294735498463\n",
            "    max_percent_error: 61.94340866612694\n",
            "    mean_percent_error: 13.95774713396101\n",
            "    median_percent_error: 7.26100598805389\n",
            "    percent_within_5: 50.0\n",
            "    percent_within_10: 50.0\n",
            "\n",
            "Training medium range bias correction model...\n",
            "  Average bias in medium range: -0.27 MPa\n",
            "  Max bias in medium range: 4.19 MPa\n",
            "\n",
            "  Medium Range Before Correction:\n",
            "    r2: 0.40361021072949044\n",
            "    rmse: 4.865222516755333\n",
            "    mae: 2.9345654302755646\n",
            "    max_percent_error: 45.986647482791845\n",
            "    mean_percent_error: 6.127001566461596\n",
            "    median_percent_error: 3.6224732416142573\n",
            "    percent_within_5: 63.1578947368421\n",
            "    percent_within_10: 89.47368421052632\n",
            "\n",
            "  Medium Range After Correction:\n",
            "    r2: 0.4866803327832697\n",
            "    rmse: 4.513688253398125\n",
            "    mae: 2.6772899444116747\n",
            "    max_percent_error: 44.724048601111875\n",
            "    mean_percent_error: 5.614545214434526\n",
            "    median_percent_error: 3.223222858329744\n",
            "    percent_within_5: 63.1578947368421\n",
            "    percent_within_10: 91.22807017543859\n",
            "\\n--- Training Meta-Learner ---\n",
            "\\nTraining enhanced non-linear meta-learner ensemble...\n",
            "Meta-features created with shape: (206, 49)\n",
            "\\nMeta-Learner Metrics:\n",
            "  r2: 0.9793515881703287\n",
            "  rmse: 2.4524702700181633\n",
            "  mae: 1.28960942942966\n",
            "  max_percent_error: 40.20145089330898\n",
            "  mean_percent_error: 4.34640045082368\n",
            "  median_percent_error: 2.0983255572708925\n",
            "  percent_within_5: 73.7864077669903\n",
            "  percent_within_10: 89.80582524271846\n",
            "\\n--- Saving Model ---\n",
            "Enhanced CatBoost models saved to models/enhanced_catboost_model.joblib\n",
            "✅ Feature statistics saved: {'total_cementitious_mean': np.float64(409.2482524271844), 'total_cementitious_std': 92.78329015939812, 'water_cement_ratio_mean': np.float64(0.7482685847867987), 'water_cement_ratio_std': 0.3140053883466934}\n",
            "\\n✅ Training pipeline complete. Model is saved and ready for deployment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TEST\n",
        "PREDICTION**"
      ],
      "metadata": {
        "id": "C8rwmMRcc9bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- In-Colab Prediction Test ---\n",
        "\n",
        "# 1. Define the exact input data that was previously incorrect\n",
        "test_data = {\n",
        "    'Cement (component 1)(kg in a m^3 mixture)': 332.0,\n",
        "    'Blast Furnace Slag (component 2)(kg in a m^3 mixture)': 142.5,\n",
        "    'Fly Ash (component 3)(kg in a m^3 mixture)': 0.0,\n",
        "    'Water  (component 4)(kg in a m^3 mixture)': 228.0,\n",
        "    'Superplasticizer (component 5)(kg in a m^3 mixture)': 0,\n",
        "    'Coarse Aggregate  (component 6)(kg in a m^3 mixture)': 932.0,\n",
        "    'Fine Aggregate (component 7)(kg in a m^3 mixture)': 594.0,\n",
        "    'Age (day)': 270.0\n",
        "}\n",
        "\n",
        "# 2. Convert the data into a pandas DataFrame\n",
        "input_df = pd.DataFrame([test_data])\n",
        "\n",
        "# 3. Use the trained predictor object to make a prediction\n",
        "final_prediction = predictor.predict(input_df)\n",
        "\n",
        "# 4. Print the final result for verification\n",
        "print(\"--- TEST PREDICTION RESULT ---\")\n",
        "print(f\"✅ Predicted Compressive Strength: {final_prediction[0]:.2f} MPa\")"
      ],
      "metadata": {
        "id": "BehGPSwoHRIW",
        "outputId": "b0c8d208-6184-48ac-d98e-a243a7e828c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Using stored feature statistics: {'total_cementitious_mean': np.float64(409.2482524271844), 'total_cementitious_std': 92.78329015939812, 'water_cement_ratio_mean': np.float64(0.7482685847867987), 'water_cement_ratio_std': 0.3140053883466934}\n",
            "✅ Final meta feature shape: (1, 49)\n",
            "📦 Meta-learner expects: 49 features\n",
            "--- TEST PREDICTION RESULT ---\n",
            "✅ Predicted Compressive Strength: 40.57 MPa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PLOTS**"
      ],
      "metadata": {
        "id": "o3iY5XhK8WS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Load your results data (assuming you've saved it from the model)\n",
        "# You can replace this with actual loading from your saved files\n",
        "# If you don't have the file, create placeholder data\n",
        "try:\n",
        "    results_df = pd.read_csv('enhanced_catboost_results.csv')\n",
        "except FileNotFoundError:\n",
        "    # Create dummy data if file doesn't exist\n",
        "    print(\"Results file not found. Creating placeholder data for visualization.\")\n",
        "    np.random.seed(42)\n",
        "    n_samples = 206\n",
        "\n",
        "    actual = np.concatenate([\n",
        "        np.random.uniform(5, 20, size=int(n_samples*0.19)),  # very_low\n",
        "        np.random.uniform(20, 40, size=int(n_samples*0.44)),  # low\n",
        "        np.random.uniform(40, 60, size=int(n_samples*0.28)),  # medium\n",
        "        np.random.uniform(60, 85, size=n_samples - int(n_samples*0.19) - int(n_samples*0.44) - int(n_samples*0.28))  # high\n",
        "    ])\n",
        "\n",
        "    # Base model predictions with some error\n",
        "    base_pred = actual * np.random.normal(1, 0.11, size=n_samples)\n",
        "    base_error = np.abs((base_pred - actual) / actual * 100)\n",
        "\n",
        "    # Ensemble model predictions with less error\n",
        "    ens_pred = actual * np.random.normal(1, 0.06, size=n_samples)\n",
        "    ens_error = np.abs((ens_pred - actual) / actual * 100)\n",
        "\n",
        "    # Assign strength ranges\n",
        "    ranges = []\n",
        "    for a in actual:\n",
        "        if a < 20:\n",
        "            ranges.append('very_low')\n",
        "        elif a < 40:\n",
        "            ranges.append('low')\n",
        "        elif a < 60:\n",
        "            ranges.append('medium')\n",
        "        else:\n",
        "            ranges.append('high')\n",
        "\n",
        "    # Create DataFrame\n",
        "    results_df = pd.DataFrame({\n",
        "        'Actual_Strength': actual,\n",
        "        'Deep_CatBoost_Prediction': base_pred,\n",
        "        'Meta_Learner_Prediction': ens_pred,\n",
        "        'Strength_Range': ranges,\n",
        "        'Deep_CatBoost_Error_Pct': base_error,\n",
        "        'Meta_Learner_Error_Pct': ens_error,\n",
        "        'Error_Improvement': base_error - ens_error\n",
        "    })\n",
        "\n",
        "# Set a consistent style for all plots\n",
        "# Use a style that's available in current matplotlib\n",
        "plt.style.use('default')\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "sns.set_palette(\"colorblind\")\n",
        "colors = {'very_low': '#FF9999', 'low': '#FFCC99', 'medium': '#99CCFF', 'high': '#99FF99'}\n",
        "\n",
        "# Create figure directory if it doesn't exist\n",
        "import os\n",
        "if not os.path.exists('methodology_figures'):\n",
        "    os.makedirs('methodology_figures')\n",
        "\n",
        "# 1. FEATURE IMPORTANCE PLOT\n",
        "def plot_feature_importance():\n",
        "    # Assuming you have feature importance saved from your model\n",
        "    # Replace this with loading your actual feature importance data\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'Feature': ['very_early_strength', 'water_cementitious_ratio', 'Blast Furnace Slag', 'Water',\n",
        "                   'high_correction', 'total_cementitious', 'very_low_correction', 'slump_indicator',\n",
        "                   'maturity_index', 'Fly Ash'],\n",
        "        'Importance': [22.02, 13.58, 4.19, 4.04, 3.38, 3.28, 3.23, 3.19, 3.17, 2.86]\n",
        "    })\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
        "    plt.title('Top 10 Features by Importance', fontsize=16)\n",
        "    plt.xlabel('Importance Score', fontsize=14)\n",
        "    plt.ylabel('Feature', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('methodology_figures/feature_importance.png', dpi=300)\n",
        "    plt.close()\n",
        "    print(\"Created: Feature Importance Plot\")\n",
        "\n",
        "# 2. PREDICTED VS ACTUAL PLOT WITH ERROR BANDS\n",
        "def plot_predicted_vs_actual():\n",
        "    # Create figure\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Scatter plot - colored by strength range\n",
        "    for range_name in ['very_low', 'low', 'medium', 'high']:\n",
        "        range_data = results_df[results_df['Strength_Range'] == range_name]\n",
        "        plt.scatter(range_data['Actual_Strength'],\n",
        "                   range_data['Meta_Learner_Prediction'],\n",
        "                   alpha=0.7, label=range_name.replace('_', ' ').title(),\n",
        "                   color=colors[range_name], s=60)\n",
        "\n",
        "    # Perfect prediction line\n",
        "    max_val = max(results_df['Actual_Strength'].max(), results_df['Meta_Learner_Prediction'].max())\n",
        "    plt.plot([0, max_val], [0, max_val], 'k--', label='Perfect Prediction')\n",
        "\n",
        "    # 10% error bands\n",
        "    x = np.linspace(0, max_val, 100)\n",
        "    plt.fill_between(x, x*0.9, x*1.1, alpha=0.1, color='gray', label='±10% Error Band')\n",
        "\n",
        "    plt.title('Predicted vs Actual Concrete Strength', fontsize=16)\n",
        "    plt.xlabel('Actual Strength (MPa)', fontsize=14)\n",
        "    plt.ylabel('Predicted Strength (MPa)', fontsize=14)\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('methodology_figures/predicted_vs_actual.png', dpi=300)\n",
        "    plt.close()\n",
        "    print(\"Created: Predicted vs Actual Plot\")\n",
        "\n",
        "# 3. ERROR DISTRIBUTION BY STRENGTH RANGE\n",
        "def plot_error_distribution():\n",
        "    # Calculate percent errors for both models\n",
        "    results_df['Base_Percent_Error'] = results_df['Deep_CatBoost_Error_Pct']\n",
        "    results_df['Ensemble_Percent_Error'] = results_df['Meta_Learner_Error_Pct']\n",
        "\n",
        "    # Prepare data for plotting\n",
        "    error_data = []\n",
        "    for range_name in ['very_low', 'low', 'medium', 'high']:\n",
        "        range_data = results_df[results_df['Strength_Range'] == range_name]\n",
        "\n",
        "        # Base model errors\n",
        "        error_data.append({\n",
        "            'Strength Range': range_name.replace('_', ' ').title(),\n",
        "            'Error (%)': range_data['Base_Percent_Error'].mean(),\n",
        "            'Model': 'Base Model'\n",
        "        })\n",
        "\n",
        "        # Ensemble model errors\n",
        "        error_data.append({\n",
        "            'Strength Range': range_name.replace('_', ' ').title(),\n",
        "            'Error (%)': range_data['Ensemble_Percent_Error'].mean(),\n",
        "            'Model': 'Ensemble Model'\n",
        "        })\n",
        "\n",
        "    error_df = pd.DataFrame(error_data)\n",
        "\n",
        "    # Create plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(x='Strength Range', y='Error (%)',\n",
        "                hue='Model', data=error_df, palette=['#4472c4', '#70ad47'])\n",
        "\n",
        "    plt.title('Mean Percentage Error by Strength Range', fontsize=16)\n",
        "    plt.xlabel('Concrete Strength Range', fontsize=14)\n",
        "    plt.ylabel('Mean Percentage Error (%)', fontsize=14)\n",
        "    plt.legend(title='', fontsize=12)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('methodology_figures/error_by_range.png', dpi=300)\n",
        "    plt.close()\n",
        "    print(\"Created: Error Distribution Plot\")\n",
        "\n",
        "# 4. MODEL PERFORMANCE COMPARISON\n",
        "def plot_performance_metrics():\n",
        "    # Calculate metrics from results if available\n",
        "    try:\n",
        "        base_r2 = r2_score(results_df['Actual_Strength'], results_df['Deep_CatBoost_Prediction'])\n",
        "        base_rmse = np.sqrt(mean_squared_error(results_df['Actual_Strength'], results_df['Deep_CatBoost_Prediction']))\n",
        "        base_mae = mean_absolute_error(results_df['Actual_Strength'], results_df['Deep_CatBoost_Prediction'])\n",
        "        base_within_5 = (results_df['Deep_CatBoost_Error_Pct'] <= 5).mean() * 100\n",
        "        base_within_10 = (results_df['Deep_CatBoost_Error_Pct'] <= 10).mean() * 100\n",
        "\n",
        "        ens_r2 = r2_score(results_df['Actual_Strength'], results_df['Meta_Learner_Prediction'])\n",
        "        ens_rmse = np.sqrt(mean_squared_error(results_df['Actual_Strength'], results_df['Meta_Learner_Prediction']))\n",
        "        ens_mae = mean_absolute_error(results_df['Actual_Strength'], results_df['Meta_Learner_Prediction'])\n",
        "        ens_within_5 = (results_df['Meta_Learner_Error_Pct'] <= 5).mean() * 100\n",
        "        ens_within_10 = (results_df['Meta_Learner_Error_Pct'] <= 10).mean() * 100\n",
        "    except:\n",
        "        # Use values from the original code\n",
        "        base_r2, base_rmse, base_mae, base_within_5, base_within_10 = 0.945, 3.99, 2.54, 48.06, 74.76\n",
        "        ens_r2, ens_rmse, ens_mae, ens_within_5, ens_within_10 = 0.977, 2.61, 1.47, 69.42, 88.35\n",
        "\n",
        "    # Prepare data\n",
        "    metrics = pd.DataFrame({\n",
        "        'Metric': ['R²', 'RMSE (MPa)', 'MAE (MPa)', 'Within 5%', 'Within 10%'],\n",
        "        'Base Model': [base_r2, base_rmse, base_mae, base_within_5, base_within_10],\n",
        "        'Ensemble Model': [ens_r2, ens_rmse, ens_mae, ens_within_5, ens_within_10]\n",
        "    })\n",
        "\n",
        "    # Convert to long format for seaborn\n",
        "    metrics_long = pd.melt(metrics, id_vars=['Metric'],\n",
        "                           var_name='Model', value_name='Value')\n",
        "\n",
        "    # Create plot\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # For barplot, we need to handle the metrics separately because they have different scales\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(15, 6), sharey=False)\n",
        "\n",
        "    # Define color palette\n",
        "    palette = {'Base Model': '#4472c4', 'Ensemble Model': '#70ad47'}\n",
        "\n",
        "    # Plot each metric in its own subplot\n",
        "    for i, metric in enumerate(metrics['Metric'].unique()):\n",
        "        metric_data = metrics_long[metrics_long['Metric'] == metric]\n",
        "\n",
        "        sns.barplot(x='Model', y='Value', data=metric_data, ax=axes[i], palette=palette)\n",
        "        axes[i].set_title(metric)\n",
        "        axes[i].set_xlabel('')\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for j, p in enumerate(axes[i].patches):\n",
        "            height = p.get_height()\n",
        "            if metric == 'R²':\n",
        "                axes[i].text(p.get_x() + p.get_width()/2., height + 0.01, f'{height:.3f}',\n",
        "                            ha=\"center\", va=\"bottom\")\n",
        "            else:\n",
        "                axes[i].text(p.get_x() + p.get_width()/2., height + 0.01, f'{height:.1f}',\n",
        "                            ha=\"center\", va=\"bottom\")\n",
        "\n",
        "    plt.suptitle('Performance Comparison: Base vs Ensemble Model', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('methodology_figures/performance_comparison.png', dpi=300)\n",
        "    plt.close()\n",
        "    print(\"Created: Performance Metrics Comparison Plot\")\n",
        "\n",
        "# 5. IMPROVEMENT BY STRENGTH RANGE\n",
        "def plot_improvement_by_range():\n",
        "    # Calculate performance metrics by strength range\n",
        "    range_improvement = []\n",
        "\n",
        "    for range_name in ['very_low', 'low', 'medium', 'high']:\n",
        "        range_data = results_df[results_df['Strength_Range'] == range_name]\n",
        "\n",
        "        # Base model - within 10%\n",
        "        base_within_10 = (range_data['Deep_CatBoost_Error_Pct'] <= 10).mean() * 100\n",
        "\n",
        "        # Ensemble model - within 10%\n",
        "        ensemble_within_10 = (range_data['Meta_Learner_Error_Pct'] <= 10).mean() * 100\n",
        "\n",
        "        # Improvement percentage points\n",
        "        improvement = ensemble_within_10 - base_within_10\n",
        "\n",
        "        range_improvement.append({\n",
        "            'Strength Range': range_name.replace('_', ' ').title(),\n",
        "            'Base Model': base_within_10,\n",
        "            'Ensemble Model': ensemble_within_10,\n",
        "            'Improvement': improvement\n",
        "        })\n",
        "\n",
        "    improve_df = pd.DataFrame(range_improvement)\n",
        "\n",
        "    # Create plot\n",
        "    fig, ax1 = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    # Bar chart for base and ensemble\n",
        "    x = np.arange(len(improve_df['Strength Range']))\n",
        "    width = 0.35\n",
        "\n",
        "    ax1.bar(x - width/2, improve_df['Base Model'], width, label='Base Model', color='#4472c4')\n",
        "    ax1.bar(x + width/2, improve_df['Ensemble Model'], width, label='Ensemble Model', color='#70ad47')\n",
        "\n",
        "    # Line chart for improvement\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(x, improve_df['Improvement'], 'ro-', linewidth=2, label='Improvement')\n",
        "\n",
        "    # Add data labels for improvement\n",
        "    for i, val in enumerate(improve_df['Improvement']):\n",
        "        ax2.annotate(f'{val:.1f}pp', xy=(i, val), xytext=(0, 5),\n",
        "                    textcoords='offset points', ha='center', fontsize=10, color='red')\n",
        "\n",
        "    # Customize plot\n",
        "    ax1.set_xlabel('Strength Range', fontsize=14)\n",
        "    ax1.set_ylabel('Predictions Within 10% (%)', fontsize=14)\n",
        "    ax2.set_ylabel('Improvement (percentage points)', fontsize=14, color='red')\n",
        "\n",
        "    ax1.set_xticks(x)\n",
        "    ax1.set_xticklabels(improve_df['Strength Range'])\n",
        "\n",
        "    ax1.legend(loc='upper left')\n",
        "    ax2.legend(loc='lower right')\n",
        "\n",
        "    plt.title('Model Improvement by Strength Range', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('methodology_figures/improvement_by_range.png', dpi=300)\n",
        "    plt.close()\n",
        "    print(\"Created: Improvement by Strength Range Plot\")\n",
        "\n",
        "# 6. ERROR ANALYSIS VISUALIZATION\n",
        "def plot_error_analysis():\n",
        "    # Instead of trying to load the error analysis, we'll create synthetic data\n",
        "    # based on the values mentioned in the code\n",
        "\n",
        "    # Creating a figure with 2 subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n",
        "\n",
        "    # Plot 1: Error rate by strength range\n",
        "    strength_error_rates = {\n",
        "        'Very Low': 28.2,\n",
        "        'Low': 8.8,\n",
        "        'Medium': 5.3,\n",
        "        'High': 10.5\n",
        "    }\n",
        "\n",
        "    ax1.bar(strength_error_rates.keys(), strength_error_rates.values(),\n",
        "           color=[colors['very_low'], colors['low'], colors['medium'], colors['high']])\n",
        "\n",
        "    ax1.set_title('Error Rate by Strength Range', fontsize=14)\n",
        "    ax1.set_xlabel('Strength Range', fontsize=12)\n",
        "    ax1.set_ylabel('Error Rate (%)', fontsize=12)\n",
        "    ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Plot 2: Error correlation with features\n",
        "    error_correlations = {\n",
        "        'late_age_factor': -0.252,\n",
        "        'log_age': -0.249,\n",
        "        'sqrt_age': -0.234,\n",
        "        'maturity_index': -0.190,\n",
        "        'water_excess_indicator': 0.190\n",
        "    }\n",
        "\n",
        "    features = list(error_correlations.keys())\n",
        "    values = list(error_correlations.values())\n",
        "    bar_colors = ['#4c78a8' if v < 0 else '#72b7b2' for v in values]\n",
        "\n",
        "    # Sort by absolute value\n",
        "    sorted_indices = np.argsort(np.abs(values))[::-1]\n",
        "    sorted_features = [features[i] for i in sorted_indices]\n",
        "    sorted_values = [values[i] for i in sorted_indices]\n",
        "    sorted_colors = [bar_colors[i] for i in sorted_indices]\n",
        "\n",
        "    ax2.barh(sorted_features, sorted_values, color=sorted_colors)\n",
        "\n",
        "    ax2.set_title('Top 5 Features Correlated with Error', fontsize=14)\n",
        "    ax2.set_xlabel('Correlation Coefficient', fontsize=12)\n",
        "    ax2.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
        "    ax2.grid(axis='x', alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('methodology_figures/error_analysis.png', dpi=300)\n",
        "    plt.close()\n",
        "    print(\"Created: Error Analysis Plot\")\n",
        "\n",
        "# 7. MODEL ENSEMBLE DIAGRAM\n",
        "def create_ensemble_diagram():\n",
        "    \"\"\"\n",
        "    Create a visual representation of the ensemble structure.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    # Set up coordinates\n",
        "    y_pos = {\n",
        "        'input': 0.9,\n",
        "        'base': 0.75,\n",
        "        'specialized': 0.5,\n",
        "        'meta': 0.25,\n",
        "        'output': 0.1\n",
        "    }\n",
        "\n",
        "    # Plot components\n",
        "    plt.scatter(0.5, y_pos['input'], s=300, color='#4472c4', zorder=5)\n",
        "    plt.scatter(0.5, y_pos['base'], s=500, color='#4472c4', zorder=5)\n",
        "\n",
        "    specialized_x = [0.2, 0.4, 0.6, 0.8]\n",
        "    specialized_colors = [colors['very_low'], colors['low'], colors['medium'], colors['high']]\n",
        "\n",
        "    for i, x in enumerate(specialized_x):\n",
        "        plt.scatter(x, y_pos['specialized'], s=400, color=specialized_colors[i], zorder=5)\n",
        "\n",
        "    plt.scatter(0.5, y_pos['meta'], s=500, color='#70ad47', zorder=5)\n",
        "    plt.scatter(0.5, y_pos['output'], s=300, color='#70ad47', zorder=5)\n",
        "\n",
        "    # Add connecting lines\n",
        "    plt.plot([0.5, 0.5], [y_pos['input'], y_pos['base']], 'k-', linewidth=2)\n",
        "\n",
        "    for x in specialized_x:\n",
        "        plt.plot([0.5, x], [y_pos['base'], y_pos['specialized']], 'k-', linewidth=2)\n",
        "        plt.plot([x, 0.5], [y_pos['specialized'], y_pos['meta']], 'k-', linewidth=2)\n",
        "\n",
        "    plt.plot([0.5, 0.5], [y_pos['meta'], y_pos['output']], 'k-', linewidth=2)\n",
        "\n",
        "    # Add labels\n",
        "    plt.text(0.5, y_pos['input']+0.05, \"Input Features\", ha='center', fontsize=12)\n",
        "    plt.text(0.5, y_pos['base']+0.05, \"Deep CatBoost Base Model\", ha='center', fontsize=12)\n",
        "\n",
        "    specialized_labels = [\"Very Low\\nRange Model\", \"Low\\nRange Model\",\n",
        "                         \"Medium\\nRange Model\", \"High Range &\\nBoundary Models\"]\n",
        "\n",
        "    for i, x in enumerate(specialized_x):\n",
        "        plt.text(x, y_pos['specialized']+0.05, specialized_labels[i], ha='center', fontsize=10)\n",
        "\n",
        "    plt.text(0.5, y_pos['meta']+0.05, \"Meta-Learner Ensemble\", ha='center', fontsize=12)\n",
        "    plt.text(0.5, y_pos['output']+0.05, \"Final Prediction\", ha='center', fontsize=12)\n",
        "\n",
        "    # Remove axes\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Set title\n",
        "    plt.title('Model Ensemble Structure', fontsize=16)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('methodology_figures/ensemble_diagram.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(\"Created: Model Ensemble Diagram\")\n",
        "\n",
        "# Generate all plots\n",
        "if __name__ == \"__main__\":\n",
        "    plot_feature_importance()\n",
        "    plot_predicted_vs_actual()\n",
        "    plot_error_distribution()\n",
        "    plot_performance_metrics()\n",
        "    plot_improvement_by_range()\n",
        "    plot_error_analysis()\n",
        "    create_ensemble_diagram()\n",
        "\n",
        "    print(\"\\nAll methodology plots have been generated and saved to 'methodology_figures/' directory\")"
      ],
      "metadata": {
        "id": "RdhCaNhOjf2q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "62429da4-cb00-4a7a-e13e-b191beeb3a3a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results file not found. Creating placeholder data for visualization.\n",
            "Created: Feature Importance Plot\n",
            "Created: Predicted vs Actual Plot\n",
            "Created: Error Distribution Plot\n",
            "Created: Performance Metrics Comparison Plot\n",
            "Created: Improvement by Strength Range Plot\n",
            "Created: Error Analysis Plot\n",
            "Created: Model Ensemble Diagram\n",
            "\n",
            "All methodology plots have been generated and saved to 'methodology_figures/' directory\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/project.zip /content/ -x '/content/sample_data*'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bojXLFSa6WvU",
        "outputId": "01775f9d-aafb-4e49-fcd6-e89b9592e0e6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/ (stored 0%)\n",
            "  adding: content/.config/ (stored 0%)\n",
            "  adding: content/.config/configurations/ (stored 0%)\n",
            "  adding: content/.config/configurations/config_default (deflated 15%)\n",
            "  adding: content/.config/default_configs.db (deflated 98%)\n",
            "  adding: content/.config/.last_update_check.json (deflated 22%)\n",
            "  adding: content/.config/logs/ (stored 0%)\n",
            "  adding: content/.config/logs/2025.08.19/ (stored 0%)\n",
            "  adding: content/.config/logs/2025.08.19/13.36.57.010391.log (deflated 92%)\n",
            "  adding: content/.config/logs/2025.08.19/13.37.51.363345.log (deflated 57%)\n",
            "  adding: content/.config/logs/2025.08.19/13.37.35.369892.log (deflated 86%)\n",
            "  adding: content/.config/logs/2025.08.19/13.37.26.410292.log (deflated 58%)\n",
            "  adding: content/.config/logs/2025.08.19/13.37.50.630653.log (deflated 57%)\n",
            "  adding: content/.config/logs/2025.08.19/13.37.41.963861.log (deflated 58%)\n",
            "  adding: content/.config/.last_opt_in_prompt.yaml (stored 0%)\n",
            "  adding: content/.config/.last_survey_prompt.yaml (stored 0%)\n",
            "  adding: content/.config/config_sentinel (stored 0%)\n",
            "  adding: content/.config/active_config (stored 0%)\n",
            "  adding: content/.config/hidden_gcloud_config_universe_descriptor_data_cache_configs.db (deflated 97%)\n",
            "  adding: content/.config/gce (stored 0%)\n",
            "  adding: content/Concrete_Data.xls (deflated 74%)\n",
            "  adding: content/methodology_figures/ (stored 0%)\n",
            "  adding: content/methodology_figures/predicted_vs_actual.png (deflated 13%)\n",
            "  adding: content/methodology_figures/feature_importance.png (deflated 30%)\n",
            "  adding: content/methodology_figures/ensemble_diagram.png (deflated 28%)\n",
            "  adding: content/methodology_figures/improvement_by_range.png (deflated 17%)\n",
            "  adding: content/methodology_figures/performance_comparison.png (deflated 26%)\n",
            "  adding: content/methodology_figures/error_analysis.png (deflated 31%)\n",
            "  adding: content/methodology_figures/error_by_range.png (deflated 32%)\n",
            "  adding: content/enhanced_catboost_predictor.log (deflated 23%)\n",
            "  adding: content/catboost_info/ (stored 0%)\n",
            "  adding: content/catboost_info/learn/ (stored 0%)\n",
            "  adding: content/catboost_info/learn/events.out.tfevents (deflated 74%)\n",
            "  adding: content/catboost_info/learn_error.tsv (deflated 53%)\n",
            "  adding: content/catboost_info/test_error.tsv (deflated 55%)\n",
            "  adding: content/catboost_info/time_left.tsv (deflated 51%)\n",
            "  adding: content/catboost_info/catboost_training.json (deflated 75%)\n",
            "  adding: content/catboost_info/test/ (stored 0%)\n",
            "  adding: content/catboost_info/test/events.out.tfevents (deflated 75%)\n",
            "  adding: content/catboost_info/tmp/ (stored 0%)\n",
            "  adding: content/models/ (stored 0%)\n",
            "  adding: content/models/enhanced_catboost_model.joblib (deflated 82%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install pyngrok and set up the authentication token\n",
        "# Get your token from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "!pip install pyngrok\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "# --- IMPORTANT ---\n",
        "# Replace YOUR_AUTHTOKEN with your actual ngrok authtoken in quotes\n",
        "# Example: conf.get_default().auth_token = \"2gA...xyz\"\n",
        "conf.get_default().auth_token = \"31Yrzx3Th6F0kqbuqRp3VNRUiUB_4ACd55k7Lf2kHJfYqTkXJ\"\n",
        "\n",
        "# 2. Define the Flask App and Template (no changes here)\n",
        "from flask import Flask, request, jsonify, render_template_string\n",
        "import pandas as pd\n",
        "\n",
        "HTML_TEMPLATE = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>In-Colab Strength Predictor</title>\n",
        "    <style>\n",
        "        body { font-family: sans-serif; margin: 2em; }\n",
        "        .container { max-width: 600px; margin: auto; padding: 2em; border: 1px solid #ccc; border-radius: 10px; }\n",
        "        .form-grid { display: grid; grid-template-columns: 1fr 1fr; gap: 1em; }\n",
        "        input { width: 90%; padding: 8px; }\n",
        "        button { padding: 10px 20px; cursor: pointer; }\n",
        "        h2 { margin-top: 1em; }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <div class=\"container\">\n",
        "        <h1>In-Colab Strength Predictor</h1>\n",
        "        <div class=\"form-grid\">\n",
        "            <label>Cement: <input type=\"number\" id=\"Cement\" value=\"332\"></label>\n",
        "            <label>Blast Furnace Slag: <input type=\"number\" id=\"Blast_Furnace_Slag\" value=\"142.5\"></label>\n",
        "            <label>Fly Ash: <input type=\"number\" id=\"Fly_Ash\" value=\"0\"></label>\n",
        "            <label>Water: <input type=\"number\" id=\"Water\" value=\"228\"></label>\n",
        "            <label>Superplasticizer: <input type=\"number\" id=\"Superplasticizer\" value=\"0\"></label>\n",
        "            <label>Coarse Aggregate: <input type=\"number\" id=\"Coarse_Aggregate\" value=\"932\"></label>\n",
        "            <label>Fine Aggregate: <input type=\"number\" id=\"Fine_Aggregate\" value=\"594\"></label>\n",
        "            <label>Age (days): <input type=\"number\" id=\"Age\" value=\"270\"></label>\n",
        "        </div>\n",
        "        <br>\n",
        "        <button onclick=\"predict()\">Predict Strength</button>\n",
        "        <h2>Predicted Compressive Strength: <span id=\"result\">---</span></h2>\n",
        "    </div>\n",
        "\n",
        "    <script>\n",
        "        async function predict() {\n",
        "            const data = {\n",
        "                Cement: document.getElementById('Cement').value,\n",
        "                Blast_Furnace_Slag: document.getElementById('Blast_Furnace_Slag').value,\n",
        "                Fly_Ash: document.getElementById('Fly_Ash').value,\n",
        "                Water: document.getElementById('Water').value,\n",
        "                Superplasticizer: document.getElementById('Superplasticizer').value,\n",
        "                Coarse_Aggregate: document.getElementById('Coarse_Aggregate').value,\n",
        "                Fine_Aggregate: document.getElementById('Fine_Aggregate').value,\n",
        "                Age: document.getElementById('Age').value,\n",
        "            };\n",
        "            const response = await fetch('/predict', {\n",
        "                method: 'POST',\n",
        "                headers: { 'Content-Type': 'application/json' },\n",
        "                body: JSON.stringify(data)\n",
        "            });\n",
        "            const result = await response.json();\n",
        "            if (result.prediction) {\n",
        "                document.getElementById('result').innerText = result.prediction + ' MPa';\n",
        "            } else {\n",
        "                document.getElementById('result').innerText = 'Error: ' + result.error;\n",
        "            }\n",
        "        }\n",
        "    </script>\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return render_template_string(HTML_TEMPLATE)\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict_route_colab():\n",
        "    data_from_form = request.get_json(force=True)\n",
        "    input_data = {\n",
        "        'Cement (component 1)(kg in a m^3 mixture)': float(data_from_form.get('Cement') or 0),\n",
        "        'Blast Furnace Slag (component 2)(kg in a m^3 mixture)': float(data_from_form.get('Blast_Furnace_Slag') or 0),\n",
        "        'Fly Ash (component 3)(kg in a m^3 mixture)': float(data_from_form.get('Fly_Ash') or 0),\n",
        "        'Water  (component 4)(kg in a m^3 mixture)': float(data_from_form.get('Water') or 0),\n",
        "        'Superplasticizer (component 5)(kg in a m^3 mixture)': float(data_from_form.get('Superplasticizer') or 0),\n",
        "        'Coarse Aggregate  (component 6)(kg in a m^3 mixture)': float(data_from_form.get('Coarse_Aggregate') or 0),\n",
        "        'Fine Aggregate (component 7)(kg in a m^3 mixture)': float(data_from_form.get('Fine_Aggregate') or 0),\n",
        "        'Age (day)': float(data_from_form.get('Age') or 0)\n",
        "    }\n",
        "    input_df = pd.DataFrame([input_data])\n",
        "    final_prediction = predictor.predict(input_df)\n",
        "    output = round(float(final_prediction[0]), 2)\n",
        "    return jsonify({'prediction': output})\n",
        "\n",
        "# 3. Open a tunnel and run the app\n",
        "public_url = ngrok.connect(5000)\n",
        "print(f\" * Click this link to open the app: {public_url}\")\n",
        "app.run(port=5000)"
      ],
      "metadata": {
        "id": "A-PsY42kkPMS",
        "outputId": "afe3bf26-258e-4205-a517-a3c8a92631fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.3.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.3.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.3.0\n",
            " * Click this link to open the app: NgrokTunnel: \"https://c371c1a8df09.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Aug/2025 18:49:11] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Aug/2025 18:49:11] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Aug/2025 18:49:13] \"POST /predict HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 Using stored feature statistics: {'total_cementitious_mean': np.float64(409.2482524271844), 'total_cementitious_std': 92.78329015939812, 'water_cement_ratio_mean': np.float64(0.7482685847867987), 'water_cement_ratio_std': 0.3140053883466934}\n",
            "✅ Final meta feature shape: (1, 49)\n",
            "📦 Meta-learner expects: 49 features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5C7ApV7o90QB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO+lWx8Iq2J49nMpJLBFuiv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}